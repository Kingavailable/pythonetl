# Python-ETL学习目标

- 使用Python语言完成任务，主要是锻炼同学的`代码能力`
- 基于Python语言完成`ETL`的相关处理任务



> 我们ETL课程是独立
>
> 学不会也不影响后续的内容。





# ETL的引出

## 数据孤岛

企业的业务数据，分散到非常多的业务数据库中

如果要对这些数据，进行数据分析，不方便，因为太分散了



这个问题，就是数据分析中的：数据孤岛问题

![img](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/15/20220515091502.png)

## 数据仓库

为了解决数据孤岛，我们需要将数据，集中的存储起来，方便集中进行分析。



这种集中数据进行分析的方案，我们一般称之为：数据仓库



![img](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/15/20220515091551.png)



## ETL的引出

既然数据仓库是集中进行数据存储和分析的地方。

那么就会涉及到，如何将零散的数据，集中的输入到数据仓库中，这个集中输入数据的工作叫做：ETL



E：抽取，数据的输入

T：转换，数据的处理

L：加载，数据的输出



对于ETL来说，就是简单的将数据：从A 处理到 B



![img](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/15/20220515091715.png)



## ETL的常见实现方式

ETL实现的话方式有很多种，最常见的有2种：

- 使用工具去完成ETL的相关任务，如：Kettle、sqoop、flume、datax
  - 优势：简单、易用，配置方便，无需写代码鼠标拖拖拽拽就完成了
  - 劣势：不够灵活，特定的任务需求难以实现
- 使用编程语言去自行开发ETL系统，比如Python、Java等
  - 优势：非常自由，想怎么处理就怎么处理
  - 劣势：开发时间长



> 这两种方式在企业中都是广泛存在的。
>
> 简单的ETL任务，常用工具去实现
>
> 复杂的ETL任务，常用代码去实现



> 本次课程主要使用：Python语言去完成一个ETL的案例









## 常见的数据格式



### CSV

使用固定的分隔符，将数据分隔成多个列的一种文件格式



分隔符可以是任意字符，一般情况下会常用：`逗号` `分号`  `制表符`   `空格` 等符号



CSV的示例：



```shell
id,name,age
1,潇潇,11
2,甜甜,11
3,美美,11

# 其中，首行为CSV的header（标头）可以有也可以省略
# 逗号，就是分隔符
# 可以看到，通过逗号将其分割为3个列，id，name，age三个列


# 用其它分隔符的演示
id;name;age
1;潇潇;11
2;甜甜;11
3;美美;11
```





### CSV的Header（标头）

CSV格式是可以有标头（Header）的，用来说明具体的列的含义

![image-20220515093159156](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/15/20220515093159.png)

如上图，就是有Header的CSV

Header不是必须的，可以有，也可以没有





### JSON格式

JSON格式和Python中的字典 本质上一样的。

Python中的字典可以和JSON格式，进行无缝的切换。



示例：

```shell
{
  "name": "zhangsan", 
  "age": 10, 
  "like": ["football", "music"]
}
{
    "sites": [
    { "name":"菜鸟教程" , "url":"www.runoob.com" }, 
    { "name":"google" , "url":"www.google.com" }, 
    { "name":"微博" , "url":"www.weibo.com" }
    ]
}
# 这是一个典型的JSON，也可以说是一个典型的Python字典数据
```



> JSON或者Python字典，本质上都是Key-Value型的数据结构
>
> Key一般是字符串，表示key的名字
>
> Value是任意类型，可以是字符串、数字、list、字典



### XML格式

XML格式和JSON一样，也是Key-Value型的数据记录格式。



写法不同：

```shell
<?xml version="1.0" encoding="UTF-8"?>
<note>
  <to>Tove</to>
  <from>Jani</from>
  <heading>Reminder</heading>
  <body>Don't forget me this weekend!</body>
</note>

# 如上，是一个典型的XML格式，它可以和JSON进行无缝转换
{
  "note": [
    "to": "Tove", 
    "from": "Jani", 
    "heading": "Reminder", 
    "body": "Don't forget me this weekend!"
  ]
}
```



> XML格式一般我们很少去处理它，但是在很多的框架中，XML是常见的配置文件格式
>
> 比如Hadoop、Hive等都是使用XML作为配置文件格式





## 结构化、半结构化、非结构化数据类型



### 结构化

概念：可以用Schema描述的数据，就是结构化的数据

Schema:数据的描述，可以描述一份数据的具体属性，比如有几个列，每个列是什么含义，可以简单的认为Scheam就是类型数据库的`表结构`



可以简单认为： 可以转换成 `二维表格`的数据，就是结构化的数据。



常见的结构化数据：

- 数据库中的表
  - 满足：可以用Schema描述，也就是被表结构所描述
- CSV，同样满足被Schema描述，有表结构（有几个列，每个列是啥意思）
- Excel
  - 就是一个二维表格、就是结构化数据



> 注意：JSON不一定是结构化，具体看JSON内的内容
>
> 如果是简单的Key-Value可以是结构化（可以转换成二维表格）
>
> 如果是复杂嵌套JSON，那么就不是结构化了

> json就是半结构化数据



### 半结构化

概念：可以用Schema描述其部分内容的数据，叫做半结构化。



简单的说：半结构化数据部分内容可以描述成二维表格，但是不一定能完全描述成二维表格。



典型的半结构化数据：

- JSON
- XML



示例：

```json
{
    "name":"潇潇",
    "age":11
}

# 是结构化吗？是结构化的JSON，因为没有复杂嵌套可以被Schema（转换成二维表）描述

{
  "name":"潇潇",
  "age":11,
  "info":{
      "addr":"北京",
      "tel":13311113333,
      "hobby": ["football", "baskteball"]
  }
}
# 是结构化吗？不是，因为无法用固定Schema去描述它，只能描述部分，不能描述全部
```



> 注意：半结构化具体看内容，如果内容简单无嵌套，可以当做结构化数据处理
>
> 如果内容复杂有嵌套，一般就不能作为结构化数据处理





### 非结构化数据

概念：完全无法用Schema描述的数据，叫做非结构化数据



常见：

- markdown文本
- Word文档
- map3
- AVI、mp4







> 以后在开发中，最常见到的是：
>
> - 结构化数据处理
> - 半结构化数据处理
>
> 非结构化数据一般不会去处理









### 结构化、半结构化、非结构化和接触到的字符串、数字这些数据类型有啥区别

区别在于：描述的内容是不一样的。



字符串、数字，描述的是`一个具体的数据`的类型

比如：

- "abc"字符串
- 123数字



结构化、半结构化、非结构化描述的是`一堆数据在一起后的类型`

```shell
123,张三,123
321,王五,333

这个是结构化数据，它描述的是一堆：字符串、数字、等组合在一起形成的一份数据整体
把这个整体叫做结构化的数据
```







# ETL案例需求

![img](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/15/20220515101728.png)





本次案例，是处理的一个`零售公司`的业务数据

这个公司是卖收银机的，收银机分布在各大商超中。



用户去买东西，收银机结账后，将用户购买的信息，通过网络发送到公司的后台。



基于这个前提，公司后台就会收集到非常多的用户购买的订单数据。



所以，我们需要做的是，将这个公司的业务数据，进行ETL任务，采集到MySQL以及文件存储中。





## 哪些数据需要采集

1. 订单数据（用户购买商品，通过网络发送到后台的订单信息）
2. 商品库的数据（存储了商品信息）
3. 后台的日志数据（记录了后台的被访问信息）

![img](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/15/20220515102257.png)



如图



## 数据在哪呢

1. 订单数据（用户购买商品，通过网络发送到后台的订单信息）
   1. 存放在JSON文件内
2. 商品库的数据（存储了商品信息）
   1. 存放在后台的MySQL数据库中
3. 后台的日志数据（记录了后台的被访问信息）
   1. 存放在后台的日志文件中





## 数据采集后输出到哪里

这三种数据，采集后，都需要完成2件事情：

1. 将其写入到数据仓库（MySQL）中

   > 因为我们还没学数仓技术，课程中以MySQL代替

2. 将输入写出为CSV文件，作为数据的一个备份







> 简单总结需求：
>
> 从：JSON文件（订单数据）、MySQL数据库（商品信息）、后台日志文件（log数据）三个地方采集数据
>
> 将它们写出到：
>
> - MySQL数据库
> - CSV文件







# 需求1：JSON数据处理（订单数据）



## 说明

被采集的是JSON文件，这个文件会不定期的在某个文件夹内产生一个新的文件,`文件名不会重复`

我们的程序，`会定期执行`（比如5分钟、10分钟）



我们要做的就是，每一次执行，通过程序读取json文件处理它们

要注意的是：`已经处理过的文件不可以重复处理`



### 思考：如何做到已处理文件不重复处理

1. 已处理文件名记录到一个文件中
2. 已处理的文件名记录到数据库的表中



选择第二种方式，将已经处理过的文件信息，记录到数据库中，因为：

- 数据库存储这些记录信息，更加安全（文件很容易被误删、磁盘损坏），数据库有专业运维，数据丢失损坏的风险远小于文件
- 数据库存储记录，无论是存储、修改等都更加方便（SQL就搞定），如果是文件的话，一般需要整个文件进行覆盖才能完成修改操作没有数据库方便





### 元数据管理

我们选择将记录，放入数据库中进行保存

这个数据库，我们可以称之为：`元数据库`





元数据：状态、数据的描述。

比如，我们记录的哪些文件被处理了，这个就是程序执行中的状态，这个状态就是`元数据`



我们将元数据通过数据库进行保存，保存元数据的库我们称之为：`元数据库`



> 元数据英文：metadata





## 关于MySQL在项目中的使用

我们在项目中接触三个数据库：

- 元数据管理的库
- 提供数据的数据源库
- 数据处理的目的地库

这三个数据库，由于我们学习阶段，自己电脑里面只有一个MySQL

这三个库，都在一个MySQL里面使用

> 这是一个模拟的行为
>
> 要知道，在企业中，这三个库，很可能存在与不同的服务器（电脑）上



所以，在自己的MySQL中要创建这三个库

```sql
# 创建元数据管理库
CREATE DATABASE metadata CHARACTER SET utf8;
# 创建零售数据库(目的地)
CREATE DATABASE retail CHARACTER SET utf8;
# 创建数据源库（提供被采集数据的）
CREATE DATABASE source_data CHARACTER SET utf8;
```





## 创建项目的PyCharm工程

<img src="https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/15/20220515112257.png" alt="image-20220515112257458" style="zoom:50%;" />



- Python的版本，建议大于等于 3.8



![image-20220515112449175](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/15/20220515112449.png)

如图，创建4个Python的Package

- config;记录整个ETL工程的配置信息
- model；记录数据的模型
- test；用来做单元测试
- util；用来记录工具方法





## 基础准备之：日志模块

一个合格的程序，日志是必须要输出的。

我们借助Python的logging库，来完成日志输出

我们先编写，基础的日志模块代码，可供业务逻辑代码使用。



我们所说的日志，就是程序在运行的时候，程序当前处于什么状态，通过对外输出信息来确定。

对外输出的这个信息，就是`程序的运行日志`。

如下：

![image-20220515112912086](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/15/20220515112912.png)





### Python logging 模块

logging模块是Python自带的日志输出工具，可以方便的输出日志信息。



#### 日志级别

logging是一个 标准的日志模块，支持日志的级别控制：

- CRITICAL（FATAL）：灾难级别，如果输出这个级别的日志，表明程序完蛋了
- ERROR：错误级别，表示程序中，出现了某些错误，但是不一定影响运行，输出到日志中，供程序员分析
- WARN：警告级别，有可能有隐患，输出日志供程序员查看
- INFO：信息，普通的信息，输出到日志中，比如处理到了多少条
- DEBUG：调错，话痨级别，使用这个级别，会输出非常详细的日志内容。



在logging中，我们可以控制允许哪个级别的日志被输出。

每一个具体的级别都会有一个数字代码：

```shell
CRITICAL = 50
FATAL = CRITICAL
ERROR = 40
WARNING = 30
WARN = WARNING
INFO = 20
DEBUG = 10
NOTSET = 0
```

比如，日志级别设置为INFO（20），logging将不会输出DEBUG和NOTSET级别的日志

比如日志级别设置为WARN（30），logging将不会输出INFO、DEBUG、NOTSET级别的日志，哪怕你在代码中调用了这个级别的输出也没用。

也就是，设置多少级别，小于这个级别的日志不会被输出。



> logging模块，默认的日志级别是WARN，30

#### logging的基本使用

```python
# coding:utf8
"""
功能：学习logging模块的基本使用
"""

# 导入logging
import logging

# logging的基本使用
# 获取到一个logging的对象
logger = logging.getLogger()

# logging可以输出比如：控制台、文件中
# 让logging将日志输出到控制台
# logger对象的addHandler可以添加一个Handler对象
# handler对象就是里面记录了 具体将日志输出到什么地方
# 如果想要将日志输出到控制台，那么需要获得一个控制台的Handler对象
# 通过logging.StreamHandler()就可以获取到一个将日志输出到控制台的Handler对象
stream_handler = logging.StreamHandler()
# 通过logger对象的addHandler添加这个stream_handler，就可以将内容输出到控制台
logger.addHandler(stream_handler)   # 这句话就给logger对象添加了将日志输出到控制台的功能

# 默认的级别是warn，我们修改一下
# 可以通过logger对象的setLevel修改默认级别
logger.setLevel(10)
logger.debug("我是debug信息")
logger.info("我是info信息")
logger.warning("我是warn信息")
```



#### logging的输出格式控制

```python
# coding:utf8
"""
功能：学习logging模块的基本使用
"""

# 导入logging
import logging

# logging的基本使用
# 获取到一个logging的对象
logger = logging.getLogger()

# logging可以输出比如：控制台、文件中
# 让logging将日志输出到控制台
# logger对象的addHandler可以添加一个Handler对象
# handler对象就是里面记录了 具体将日志输出到什么地方
# 如果想要将日志输出到控制台，那么需要获得一个控制台的Handler对象
# 通过logging.StreamHandler()就可以获取到一个将日志输出到控制台的Handler对象
stream_handler = logging.StreamHandler()

# Logging可以对输出的日志的格式进行控制
# 通过logging.Formatter(日志的格式字符串定义)完成日志输出格式的控制
# 比如我想输出如下格式的日志：
# 2022-05-14 17:04:38,681 - [INFO] - json_service.py[line:15]: 采集JSON数据程序启动。。。。。。
# 上述日志的格式是：时间 - [级别] - 输出日志的代码文件[输出日志的代码行]: 日志正文
# 上诉日志的格式的格式化代码：%(asctime)s - [%(levelname)s] - %(filename)s[%(lineno)d]: %(message)s
# 通过Formatter获取格式对象
fmt = logging.Formatter(
    "%(asctime)s - [%(levelname)s] - %(filename)s[%(lineno)d]: %(message)s"
)
# 通过Handler的setFormatter方法，给stream_handler提供输出的格式控制
stream_handler.setFormatter(fmt)
# 通过logger对象的addHandler添加这个stream_handler，就可以将内容输出到控制台
logger.addHandler(stream_handler)   # 这句话就给logger对象添加了将日志输出到控制台的功能


# 默认的级别是warn，我们修改一下
# 可以通过logger对象的setLevel修改默认级别
logger.setLevel(10)
logger.debug("我是debug信息")
logger.info("我是info信息")
logger.warning("我是warn信息")
"""
附录:Formatter参数列表
%(name)s            Name of the logger (logging channel)
%(levelno)s         日志的等级数字
%(levelname)s       日志的等级字符串
%(pathname)s        日志的路径
%(filename)s        输出日志的文件名称
%(module)s          Module (name portion of filename)
%(lineno)d          输出日志的代码的行数
%(funcName)s        方法名称
%(created)f         Time when the LogRecord was created (time.time()
                    return value)
%(asctime)s         输出日志的时间
%(msecs)d           Millisecond portion of the creation time
%(relativeCreated)d Time in milliseconds when the LogRecord was created,
                    relative to the time the logging module was loaded
                    (typically at application startup time)
%(thread)d          Thread ID (if available)
%(threadName)s      Thread name (if available)
%(process)d         Process ID (if available)
%(message)s         日志的正文信息
"""
```



#### logging输出到文件中

输出到文件，只需要将Handler替换为文件Handler就可以了



logging有2个常用Handler

- StreamHandler：`logging.StreamHandler()`
- FileHandler：`logging.FileHandler(参数1，参数2，参数3)`
  - 参数1：文件路径
  - 参数2：模式，"a"是追加，"w"是覆盖
  - 参数3：编码，一般UTF-8



```python
file_handler = logging.FileHandler(
    filename="D:/dev/code/python/python-etl/logs/test.log",  # 指定输出的文件是谁
    mode="a",                                                # 可以是a 和w ，a追加 w覆盖
    encoding="UTF-8"    # 编码
)
```



### 准备logging工具方法

在config包中，新建：project_config.py

用作配置文件，将日志的相关配置项写入：

```python
# coding:utf8
"""
此文件是整个ETL案例中
所有的配置项，都设置在这个Python文件中
"""
import time

# ################## --程序运行日志的配置项-- ###################
# 配置日志输出的根目录
log_root_path = "D:/dev/code/python/python-etl/logs/"
# 配置日志输出的文件名
log_name = f'pyetl-{time.strftime("%Y%m%d-%H", time.localtime(time.time()))}.log'
"""
常见的时间格式化的格式：
%Y:4位数字的年份：2022
%m:2位数字的月份：05
%d:2位数字的日期：15
%H：24小时制的小时
%M：2位数字的分钟
%S：2位数字的秒
如果要格式化为：2022-05-15 10:05:55
%Y-%m-%d %H:%M:%S
"""
# ################## --程序运行日志的配置项-- ###################

```



在util包中，新建logging_util.py

准备日志相关的工具方法：

```python
# coding:utf8
"""
这个python文件的功能，是构建日志输出的模块
方便我们后续快速的在程序中方便的输入日志信息

Python中最常用的日志库，是一个叫做logging的模块
"""

import logging
from config import project_config as conf


# 先封装一个class，class提供基本的logger对象（没有啥属性，只有级别默认为INFO）
class Logging:
    def __init__(self, level=20):
        self.logger = logging.getLogger()
        self.logger.setLevel(level)


# 构建一个方法，我们可以通过这个方法返回所需的logger对象
def init_logger():
    # 初始化刚刚自己定义的Logging类，得到类中的logger对象
    logger = Logging().logger

    if logger.handlers:
        return logger

    # 对logger对象设置属性，比如输出到文件以及输出格式的设置
    file_handler = logging.FileHandler(
        filename=conf.log_root_path + conf.log_name,
        mode="a",
        encoding="UTF-8"
    )

    # 设置一个format输出格式
    fmt = logging.Formatter(
        "%(asctime)s - [%(levelname)s] - %(filename)s[%(lineno)d]: %(message)s"
    )

    # 将格式设置到文件的handler中
    file_handler.setFormatter(fmt)

    # 将文件输出的Handler设置给logger对象
    logger.addHandler(file_handler)

    return logger

```



### 针对logging工具方法进行单元测试

在test包中，新建：test_logging_util.py

填入：

```python
# coding:utf8
"""
测试 日志的工具方法
"""

# Python中提供了一个unittest工具包，可以用来做单元测试
import logging
from unittest import TestCase
from util import logging_util


class TestLoggingUtil(TestCase):
    def setUp(self) -> None:
        pass

    def test_get_logger(self):
        logger = logging_util.init_logger()
        result = isinstance(logger, logging.RootLogger)
        self.assertEqual(True, result)

```



### 拓展：单元测试

目的：写完一个功能方法，最好立刻测试一下

这个测试在Python中有专业的测试工具，叫做单元测试

可以使用Python自带的`unittest`包来做。



使用方式：

```python
from unittest import TestCast


class 自定义类(TestCast):
    pass

# 自定义类继承TestCast即可

在自定义类中，写单元测试方法，如下：
import logging
from unittest import TestCase
from util import logging_util


class TestLoggingUtil(TestCase):
    def setUp(self) -> None:
        pass

    def test_get_logger(self):
        logger = logging_util.init_logger()
        result = isinstance(logger, logging.RootLogger)
        self.assertEqual(True, result)
```



> 一个测试类，一般针对一个python文件，python文件中的功能方法，可以对应测试类的一个测试方法





### 结果验证

当你继承`TestCase`后，类中就有对应的 验证方法

常用的有：

```python
# 验证arg1和arg2是否相等，如果相等测试通过
self.assertEqual(arg1, arg2)

# 验证arg1和arg2是否是同一个类型
self.assertIsInstance(arg1, arg2)

# 验证arg1和arg2是否不相等，不相等测试通过
self.assertNotEqual(arg1, arg2)

# 验证arg1和arg2是否不是一个类型，不是一个类型测试通过
self.assertNotIsInstance(arg1, arg2)

# 验证arg1是否是None，是就通过
self.assertIsNone(arg1)

# 验证arg1是否小于arg2，小于就通过
self.assertLess(arg1, arg2)

# 验证arg1是否小于等于arg2，小于等于就通过
self.assertLessEqual(arg1, arg2)

# 验证arg1是否是True，是就通过
self.assertTrue(arg1)

# 验证arg1是否大于arg2，大于就通过
self.assertGreater(arg1, arg2)

# 验证arg1是否大于等于arg2，大于等于就通过
self.assertGreaterEqual(arg1, arg2)
```





## 步骤1：找出哪些文件可供处理



- 在工程根目录，创建`json_service.py`
- 在util包，创建：`file_util.py`
- 在test包，创建`test_file_util.py`



file_util.py

```python
# coding:utf8
"""
和文件处理相关的工具方法，都定义到这个Python文件中
"""
import os


def get_dir_files_list(path="./", recursion=False):
    """
    判断文件夹下面，有哪些文件
    :param path: 被判断的文件夹的路径，默认当前路径
    :param recursion: 是否递归读取，默认不递归
    :return: list对象，list里面存储的是文件的路径
    """
    # os.listdir 这个API 返回的是 你给定的path下面有哪些```文件和文件夹```
    dir_names = os.listdir(path)
    files = []      # 定义一个list，用来记录文件

    for dir_name in dir_names:
        absolute_path = f"{path}/{dir_name}"
        if not os.path.isdir(absolute_path):
            # 如果进来这个if，表明这个是：文件
            files.append(absolute_path)
        else:
            # 表明是文件夹
            if recursion:   # 如果recursion是True，表明要进到文件夹里面继续找文件
                recursion_files_list = get_dir_files_list(absolute_path, recursion=recursion)
                files += recursion_files_list   # 拼接内层的返回list到当前层的list中

    return files

```



test_file_util.py

```python
# coding:utf8
"""
针对file_util.py内的方法做单元测试
"""
import os.path
from unittest import TestCase
from util import file_util


class TestFileUtil(TestCase):
    def setUp(self) -> None:
        """等同于__init__"""
        # os.path.dirname(os.getcwd())获取当前工程的根目录
        self.project_root_path = os.path.dirname(os.getcwd())

    def test_get_dir_files_list(self):
        """
        请在工程根目录的test文件夹内建立：
        test_dir/
            inner1/
                3
                4
                inner2/
                    5
            1
            2
        的目录结构用于进行此方法的单元测试
        不递归结果应该是1和2
        递归结果应该是1, 2, 3, 4, 5
        """
        test_path = f"{self.project_root_path}/test/test_dir"
        # 先测试不递归
        result = file_util.get_dir_files_list(test_path, recursion=False)
        # result记录的是绝对路径，我们需要将文件的名字取出来
        names = []  # 定义一个list记录结果的文件名
        for i in result:
            # os.path.basename可以从路径中取出最后的文件名
            names.append(os.path.basename(i))
        # 为了避免结果的顺序产生测试失败，将names对象升序
        names.sort()        # 将 list 升序 [1 3 2], [1 2 3]
        self.assertEqual(["1", "2"], names)

        # 再测试递归
        result = file_util.get_dir_files_list(test_path, recursion=True)
        # result记录的是绝对路径，我们需要将文件的名字取出来
        names = []  # 定义一个list记录结果的文件名
        for i in result:
            # os.path.basename可以从路径中取出最后的文件名
            names.append(os.path.basename(i))
        names.sort()
        self.assertEqual(["1", "2", "3", "4", "5"], names)

```





### 构建MySQL操作的工具类

#### 先准备一些配置信息

```python
# 在project_config.py中，添加如下代码：
# ################## --JSON订单数据采集的相关配置项 start-- ###################
# 被采集的JSON数据，在哪个文件夹
json_data_root_path = "e:/pyetl-data-logs/json"


# ################## --JSON订单数据采集的相关配置项 end-- ###################

# ################## --MySQL相关配置项 start-- ###################
mysql_charset = "utf8"
# 元数据管理库的配置
metadata_host = "localhost"
metadata_user = "root"
metadata_password = "123456"
metadata_port = 3306


# 数据源数据库的配置


# 目标数据库的配置


# ################## --MySQL相关配置项 end-- ###################
```





#### 编写MySQL工具类

在util下新建：`mysql_util.py`



```python
# coding:utf8
"""
这是一个MySQL的工具类
提供操作MySQL的相关功能
提供的功能有：
- 创建MySQL的连接
- 关闭连接
- 执行SQL查询的功能，并返回查询结果
- 执行一条单独的无返回值的SQL语句（CREATE、UPDATE）
- 创建表
- 查看表是否存在
等
"""

# 实现的2种方式
# 1。不写类，我们应该将这些功能，都写成一个个的方法
# 2。我们应该写一个class 类，将这些方法，都写到类的里面
# 我们选择第二种
# 因为，我们要维护 connection（mysql的链接）
# class有一个成员变量，就是链接，
# 只要class的对象没有被销毁，那么这个连接在连接成功后，就可以持续的使用
# 如果不使用class 写到方法中，那么连接对象就是一次性的
import pymysql
from config import project_config as conf
from util.logging_util import init_logger

logger = init_logger()


class MySQLUtil:
    # __init__：构造方法（构造器），创建类对象的时候，会执行这个init
    # 链接的英文connection，一般变量可以写：conn表示connection
    def __init__(self):
        # pymysql可以通过Connection方法获取到MySQL的链接
        self.conn = pymysql.Connection(
            host=conf.metadata_host,                # 主机名 IP地址
            user=conf.metadata_user,                # 账户
            password=conf.metadata_password,        # 密码
            port=conf.metadata_port,                # 端口
            charset=conf.mysql_charset,             # 编码
            # autocommit如果是True，当你使用游标执行sql后，直接生效
            # autocommit如果是False，当你使用游标执行sql后，不会立刻生效
            # 而是需要用链接对象调用.commit()的方法才会生效
            autocommit=False                        # 自动提交
        )
        # 输出一条info级别的日志
        logger.info(f"构建完成到{conf.metadata_host}:{conf.metadata_port}的数据库连接...")

    def close_conn(self):
        if self.conn:   # 如果连接还正常
            self.conn.close()

    def query(self, sql):
        """
        执行指定的SQL语句查询，并返回查询结果
        :return:
        """
        # 执行SQL，需要先拿到游标
        cursor = self.conn.cursor()
        # 执行SQL
        cursor.execute(sql)
        # 通过游标获取执行结果
        result = cursor.fetchall()
        # 关闭游标
        cursor.close()
        # 输出一份日志
        logger.info(f"执行查询的SQL语句完成，查询结果有{len(result)}条, 执行的查询SQL是：{sql}")

        return result

    def select_db(self, db):
        """选择数据库，就是SQL中的use功能"""
        self.conn.select_db(db)

    def execute(self, sql):
        """直接执行一条SQL语句， 不理会返回值"""
        # 拿到游标
        cursor = self.conn.cursor()
        # 执行SQL
        cursor.execute(sql)


        logger.debug(f"执行了一条SQL：{sql}")

        if not self.conn.get_autocommit():
            # 表示自动提交为False
            self.conn.commit()
        # 关闭游标
        cursor.close()

    def execute_without_autocommit(self, sql):
        """
        直接执行一条SQL语句，不理会返回值
        不会判断自动提交，只执行不会commit
        :param sql:
        :return:
        """
        # 拿到游标
        cursor = self.conn.cursor()
        # 执行SQL
        cursor.execute(sql)     # 这条SQL能否执行，取决于自动提交参数，是True就能执行，是False就暂缓

        logger.debug(f"执行了一条SQL：{sql}")

    def check_table_exists(self, db_name, table_name):
        """
        检查给定的数据库下，给定的表，是否存在
        :param db_name:     被检查的数据库
        :param table_name:  被检查的表名字
        :return: True存在，False不存在
        """
        # 切换数据库
        self.conn.select_db(db_name)
        # 查询
        # SQL查询结果是 元组嵌套元组
        # 假设，有2个表，table1和table2，比如SHOW TABLES的结果
        # ((table1, ), (table2, ))
        result = self.query("SHOW TABLES")

        return (table_name, ) in result

    def check_table_exists_and_create(self, db_name, table_name, create_cols):
        """
        检查表是否存在，如果不存在，就创建它
        :param db_name:     数据库名字
        :param table_name:  被创建的表名字
        :param create_cols: 建表语句的列信息
        :return:
        """
        # 先判断表是否存在
        if not self.check_table_exists(db_name, table_name):
            # 进来if表示，表不存在

            # 准备建表的SQL语句
            create_sql = f"CREATE TABLE {table_name}({create_cols})"
            # 执行建表语句
            self.conn.select_db(db_name)
            self.execute(create_sql)

            logger.info(f"在数据库：{db_name}中创建了表：{table_name}完成。建表语句是：{create_sql}")
        else:
            logger.info(f"数据库：{db_name}中，表{table_name}已经存在，创建表的操作跳过。")


def get_processed_files(db_util,
                        db_name=conf.metadata_db_name,
                        table_name=conf.metadata_file_monitor_table_name,
                        create_cols=conf.metadata_file_monitor_table_create_cols):
    """
    查询已经被处理过的文件列表
    通过被查询的表，如果不存在，那么会先创建它
    :param db_util:
    :return:
    """
    # 已经处理过的数据的信息记录，我们存入到数据库：metadata
    # 存入到表：file_monitor中
    db_util.select_db(db_name)
    db_util.check_table_exists_and_create(
        db_name,  # 数据库名
        table_name,  # 表名
        create_cols  # 建表语句列信息
    )
    # CURRENT_TIMESTAMP 表示当前时间戳，如果设置为列的默认值，如果你插入数据不插入这个列的话，会自动填入当前的时间
    result = db_util.query(f"SELECT file_name FROM {table_name}")

    # 将SQL查询结果 转换成List返回
    processed_files = []
    for r in result:
        processed_files.append(r[0])

    return processed_files

```



#### MySQL工具类的单元测试

在test内新建：`test_mysql_util.py`

```python
# coding:utf8
"""
测试刚刚写好的MySQL工具类的一系列功能
"""
from unittest import TestCase
from util.mysql_util import MySQLUtil, get_processed_files
from config import project_config as conf

# 要继承TestCase
class TestMySQLUtil(TestCase):
    def setUp(self) -> None:
        # setUp等同于__init__
        self.db_util = MySQLUtil()

    def test_query(self):
        # 测试MySQLUtil中的 query方法
        # 不使用mysql中已存在的表，解耦合，确保单元测试的独立性
        # 耦合：和其它的东西关联的太深
        # 解耦合：解除和其它东西的深度关联，确保自身的独立
        # 要自己建个表，自己插入数据，使用被测试的query来验证它

        self.db_util.select_db("test")
        self.db_util.check_table_exists_and_create(
            "test",
            "for_unit_test",
            "id int primary key, name varchar(255)"
        )

        self.db_util.execute("TRUNCATE for_unit_test")

        self.db_util.execute(
            "INSERT INTO for_unit_test VALUES(1, '潇潇'), (2, '甜甜')"
        )

        # 数据准备好了，准备测试
        result = self.db_util.query("SELECT * FROM for_unit_test ORDER BY id")
        expected = ((1, "潇潇"), (2, "甜甜"))
        self.assertEqual(expected, result)

        # 清理单元测试的残留
        self.db_util.execute("DROP TABLE for_unit_test")

        self.db_util.close_conn()

    def test_execute_without_autocommit(self):
        # 设置AutoCommit为True
        self.db_util.conn.autocommit(True)
        self.db_util.select_db("test")
        self.db_util.check_table_exists_and_create(
            "test",
            "for_unit_test2",
            "id int primary key, name varchar(255)"
        )

        self.db_util.execute("TRUNCATE for_unit_test2")

        self.db_util.execute_without_autocommit(
            "INSERT INTO for_unit_test2 VALUES(1, '潇潇')"
        )

        # 数据准备好了，准备测试
        result = self.db_util.query("SELECT * FROM for_unit_test2 ORDER BY id")
        expected = ((1, "潇潇"), )
        self.assertEqual(expected, result)
        self.db_util.close_conn()

        # 设置AutoCommit为False
        new_util = MySQLUtil()
        new_util.select_db("test")
        new_util.conn.autocommit(False)
        new_util.execute_without_autocommit(
            "INSERT INTO for_unit_test2 VALUES(2, '甜甜')"
        )
        new_util.close_conn()

        new_util2 = MySQLUtil()
        new_util2.select_db("test")
        result = new_util2.query("SELECT * FROM for_unit_test2 ORDER BY id")
        expected = ((1, "潇潇"), )
        self.assertEqual(expected, result)

        # 清理单元测试的残留
        new_util2.execute("DROP TABLE for_unit_test2")
        new_util2.close_conn()

    def test_get_processed_files(self):
        """
        测试获取已经被处理过的文件列表功能的单元测试
        保证独立性，自备表和数据
        """
        # 准备表
        self.db_util.check_table_exists_and_create(
            "test", # 选择测试库用于单元测试
            "test_file_monitor",
            conf.metadata_file_monitor_table_create_cols
        )
        # 清空表
        self.db_util.execute("TRUNCATE test_file_monitor")

        # 准备测试数据
        self.db_util.execute(
            """
            INSERT INTO test_file_monitor VALUES(1, 'e:/data.log', 1024, '2000-01-01 10:00:00')
            """)

        # 通过我们写的工具方法，去查询文件列表
        result = get_processed_files(self.db_util, "test", "test_file_monitor")
        expected = ['e:/data.log']

        # 验证
        self.assertEqual(expected, result)

        # 清理残留
        self.db_util.execute("DROP TABLE test_file_monitor")
        self.db_util.close_conn()

```





### 编写对比的工具方法

- 从文件夹中可以读取到待处理的文件

- 从MySQL中可以读取到已经处理过的文件

现在需要两个结果进行对比，找出哪些文件没有被处理



编写一个工具方法，用来对比两个结果集



在`file_util.py`下，追加如下代码：

```python
def get_new_by_compare_lists(a_list, b_list):
    """
    从两个list中进行对比
    找出：B中有的而A中没有的
    比如：
    A： [1]
    B：[1, 2, 3]
    结果是 ：[2 ,3]
    :param a_list: A数据集
    :param b_list: B数据集
    :return: list，存放B中有的而A中没有的数据
    """
    # 创建一个空list，用来存放结果集
    new_list = []

    # 开始进行对比，for循环B数据集，验证B数据集中的数据，是否在A里面存在
    for i in b_list:
        if i not in a_list:     # 表明B中的这个数据，在A中不存在
            new_list.append(i)

    return new_list
```



并且对其进行单元测试，在`test_file_util.py`内，新增如下单元测试功能：

```python
def test_new_by_compare_lists(self):
    """测试new_by_compare_lists方法"""
    a_list = ['e:/a.txt', 'e:/b.txt']
    b_list = ['e:/a.txt', 'e:/b.txt', 'e:/c.txt', 'e:/d.txt']
    result = file_util.get_new_by_compare_lists(a_list, b_list)
    self.assertEqual(['e:/c.txt', 'e:/d.txt'], result)
```





### 回到业务主逻辑

打开`json_service.py`文件，填入如下内容：

```python
# coding:utf8
"""
采集JSON数据（订单数据）到
MySQL和CSV的功能代码逻辑
"""

from util.logging_util import init_logger
from util import file_util as fu
from config import project_config as conf
from util.mysql_util import MySQLUtil, get_processed_files

# 步骤1：找出哪些文件可以供我们处理

# 首先，获取到logger对象，用于后续输出日志用
logger = init_logger()
logger.info("读取JSON数据处理，程序开始执行了......")

# 判断，JSON数据所在的文件夹下面有哪些文件可以供我们读取
files = fu.get_dir_files_list(conf.json_data_root_path)
logger.info(f"判断json的文件夹，发现有如下文件：{files}")

# 判断，这些文件哪些是可以处理的，哪些是已经处理过的
db_util = MySQLUtil()
# 获取哪些文件是已经处理过的
processed_files = get_processed_files(db_util)
logger.info(f"查询MySQL，找到有如下文件已经被处理过了：{processed_files}")

# 对比files和processed_files，找出没有被处理的文件供我们使用
# 调用工具方法，对比它们
need_to_process_files = fu.get_new_by_compare_lists(processed_files, files)
logger.info(f"经过对比mysql元数据库，找出如下文件供我们处理：{need_to_process_files}")

```



> 至此，我们就可以找出哪些文件供处理了





## 步骤2



### 拓展：用模型记录数据

一个JSON数据，如：`{"name":"张三", "age":11, "address": "北京市海淀区"}`

- 可以使用Python的字典去存储数据

- 也可以使用模型（类、class）去存储数据



如下：

```python
# 用模型承载数据，模型就是一个class，一个class的实例，就是一条数据
class Person:
    def __init__(self, name, age, address):
        self.name = name            # 成员变量，属性
        self.age = age
        self.address = address

    def to_csv(self, sep=","):
        return f"{self.name}{sep}{self.age}{sep}{self.address}"

    def generate_insert_sql(self):
        return f"INSERT INTO person VALUES('{self.name}', {self.age}, '{self.address}')"


p1 = Person("张三", 11, "北京市海淀区")
p2 = Person("王五", 13, "北京市昌平区")
```

通过class Person，就能够承载如上的JSON数据。

不仅仅能够存数据，还能在模型（class）内，提供方法，方便的对数据操作。



> 使用模型和使用字典，都能完成数据处理的功能。
>
> 但是使用模型更加高级，更加符合面向对象的思维（封装）
>
> 也就是将一条数据看做一个对象







```json
{
  "orderID":"1233123123123312"
  "storeProvince": "湖南省",
  "payType": "wechat",
  "storeName": "湖南平价特产总汇",
  "dateTS": 1542458768000,
  "product": [
    {
      "count": 3,
      "name": "伊利优酸乳草莓味250ml",
      "unitID": 3,
      "barcode": "6907992100012",
      "pricePer": 2.5,
      "retailPrice": 2.5,
      "tradePrice": 1.8,
      "categoryID": 11
    },
    {
      "count": 3,
      "name": "巧克力面包",
      "unitID": 4,
      "barcode": "6971518660038",
      "pricePer": 3,
      "retailPrice": 3,
      "tradePrice": 1.5,
      "categoryID": 1
    }
  ],
}
```



### 扩展：时间戳

#### 时间戳的概念

用一个数字来记录时间，这个数字是从UTC时间：1970-01-01 00:00:00开始



比如，如果是一个`秒级`的时间戳数字，30，那么它对应的时间是：`1970-01-01 00:00:30`

也就是指：从`1970-01-01 00:00:00`开始，30秒后的时间。



比如，如果一个`秒级`时间戳是：75，对应的时间：`1970-01-01 00:01:15`



比如，如果一个`毫秒级`的时间戳是：80000，对应的时间是：`1970-01-01 00:01:20`



#### 注意

当下2022年左右，我们

- 秒级时间戳，在10位长度
- 毫秒级时间戳，在13位长度





#### Python中时间戳转换

> 方式太多了
>
> 怎么记呢：不要记，靠百度就可以了

##### 1. 获取当前时间

```
import time

now = time.strftime("%Y-%m-%d %H:%M:%S")  # 获取当前时间，并用指定格式显示
print(now)  # 打印：2021-06-29 11:27:14

常见的时间格式化的格式：
%Y:4位数字的年份：2022
%m:2位数字的月份：05
%d:2位数字的日期：15
%H：24小时制的小时
%M：2位数字的分钟
%S：2位数字的秒
如果要格式化为：2022-05-15 10:05:55
%Y-%m-%d %H:%M:%S
```

##### 2. 获取当前时间戳

```python
# 获取当前秒级时间戳
import time
# 秒级时间戳
ts = int(time.time())

# 获取毫秒级时间戳
ts = time.time()
int(round(ts * 1000))
```



##### 3. 将时间戳转换成日期

```python
import time

ts = 1652840528
timeArray = time.localtime(ts)  # 转换为tm格式：time.struct_time(tm_year=2021, tm_mon=6, tm_mday=28, tm_hour=1, tm_min=0, tm_sec=0, tm_wday=0, tm_yday=179, tm_isdst=0)
timeStyle = time.strftime("%Y-%m-%d %H:%M:%S", timeArray)  # 转化为指定格式：2021-06-28 01:00:00
print(timeStyle)
```



### 定义一些工具方法

- 将时间戳转换为日期字符串的方法
- 检查字符串是否是空内容的方法
- 检查字符串是否是空内容，如果是空内容返回字符串"NULL"非空内容返回字符串："'内容本身'"的方法



**在util中创建str_util.py**

```python
# coding:utf8
"""
专用于字符串的相关工具代码
"""


def check_null(data):
    """
    检查传入的字符串，是否为无意义内容，如果是就返回True，否则返回False
    无意义：字符串为空字符串、内容是None、内容是null、内容是undefined
    :param data: 传入的被检查的字符串内容
    :return: True 无意义，False有意义
    """
    if not data:
        # data这个对象， 真的是None 直接返回无意义
        return True
    """
    None 在if判断中 表示False
    if None  ==  if False
    
    如果data是None
    if not data == if not False = 会进入if循环
    """

    # 统一转小写，避免大小写问题
    # 调用字符串的lower() 方法，将字符串转换成全小写
    data = data.lower()
    # 判断是否无意义
    if data == "none" or data == "" or data == "null" or data == "undefined":
        # 满足任意一个要求，就是无意义，返回True
        return True
    return False


def check_str_null_and_transform_to_sql_null(data):
    """
    检查字符串，如果是空内容，就返回SQL意义上的NULL（插入的SQL语句中会插入真正的NULL）
    如果是有意义的内容，返回 '内容本身'
    :param data:
    :return:
    """
    if check_null(str(data)):
        # 如果进入if，表示内容无意义，返回SQL意义上 的NULl
        return "NULL"
    else:
        # 内容有意义，返回'内容本身'
        return f"'{data}'"

```



**在util中创建time_util.py**

```python
# coding:utf8
"""
此文件是和时间相关的工具代码集合
"""
import time


def ts10_to_date_str(ts, format_string="%Y-%m-%d %H:%M:%S"):
    """
    将10位（秒级）的时间戳，转换成给定的日期格式
    :param ts: 即将被转换的时间戳数字
    :param format_string: 转换后的日期格式，默认是：2022-01-01 10:00:00的格式
    :return: 转换完成后的日期字符串
    """
    # 将时间戳转换成时间数组（中转格式）
    time_array = time.localtime(ts)
    # 将时间数组格式化为指定的格式
    return time.strftime(format_string, time_array)


def ts13_to_date_str(ts, format_string="%Y-%m-%d %H:%M:%S"):
    """
    将13位（毫秒级）的时间戳，转换成给定的日期格式
    将毫秒转换成秒，会损失毫秒级的精度，这个损失是设计内的。
    :param ts: 即将被转换的时间戳数字
    :param format_string: 转换后的日期格式，默认是：2022-01-01 10:00:00的格式
    :return: 转换完成后的日期字符串
    """
    # 将13位的时间戳 规范为10位时间戳
    ts10 = int(ts / 1000)
    # 调用ts10_to_date_str方法完成转换即可   2022-01-01 10:00:00， 20220101100000
    return ts10_to_date_str(ts10, format_string=format_string)

```



### 模型设计

我们的JSON数据，里面包含了一条订单的信息以及本订单所售卖的商品信息

针对JSON数据我们设计2个模型：

- 订单数据模型（除商品信息外的信息），起名叫做：`OrdersModel`
- 订单详情数据模型（仅包含商品信息），起名叫做：`OrdersDetailModel`



### 创建模型



#### 在配置文件中新增关于mysql表的相关配置

```python
# 在project_config.py内追加如下代码：
# 目标数据库的相关配置
target_host = metadata_host
target_user = metadata_user
target_password = metadata_password
target_port = metadata_port
target_db_name = "retail"

# JSON数据采集后，写入MySQL，存储订单相关的表，表名是：
target_orders_table_name = "orders"
# orders表的建表语句的列信息
target_orders_table_create_cols = \
    f"order_id VARCHAR(255) PRIMARY KEY, " \
    f"store_id INT COMMENT '店铺ID', " \
    f"store_name VARCHAR(30) COMMENT '店铺名称', " \
    f"store_status VARCHAR(10) COMMENT '店铺状态(open,close)', " \
    f"store_own_user_id INT COMMENT '店主id', " \
    f"store_own_user_name VARCHAR(50) COMMENT '店主名称', " \
    f"store_own_user_tel VARCHAR(15) COMMENT '店主手机号', " \
    f"store_category VARCHAR(10) COMMENT '店铺类型(normal,test)', " \
    f"store_address VARCHAR(255) COMMENT '店铺地址', " \
    f"store_shop_no VARCHAR(255) COMMENT '店铺第三方支付id号', " \
    f"store_province VARCHAR(10) COMMENT '店铺所在省', " \
    f"store_city VARCHAR(10) COMMENT '店铺所在市', " \
    f"store_district VARCHAR(10) COMMENT '店铺所在行政区', " \
    f"store_gps_name VARCHAR(255) COMMENT '店铺gps名称', " \
    f"store_gps_address VARCHAR(255) COMMENT '店铺gps地址', " \
    f"store_gps_longitude VARCHAR(255) COMMENT '店铺gps经度', " \
    f"store_gps_latitude VARCHAR(255) COMMENT '店铺gps纬度', " \
    f"is_signed TINYINT COMMENT '是否第三方支付签约(0,1)', " \
    f"operator VARCHAR(10) COMMENT '操作员', " \
    f"operator_name VARCHAR(50) COMMENT '操作员名称', " \
    f"face_id VARCHAR(255) COMMENT '顾客面部识别ID', " \
    f"member_id VARCHAR(255) COMMENT '顾客会员ID', " \
    f"store_create_date_ts TIMESTAMP COMMENT '店铺创建时间', " \
    f"origin VARCHAR(255) COMMENT '原始信息(无用)', " \
    f"day_order_seq INT COMMENT '本订单是当日第几单', " \
    f"discount_rate DECIMAL(10, 5) COMMENT '折扣率', " \
    f"discount_type TINYINT COMMENT '折扣类型', " \
    f"discount DECIMAL(10, 5) COMMENT '折扣金额', " \
    f"money_before_whole_discount DECIMAL(10, 5) COMMENT '折扣前总金额', " \
    f"receivable DECIMAL(10, 5) COMMENT '应收金额', " \
    f"erase DECIMAL(10, 5) COMMENT '抹零金额', " \
    f"small_change DECIMAL(10, 5) COMMENT '找零金额', " \
    f"total_no_discount DECIMAL(10, 5) COMMENT '总价格(无折扣)', " \
    f"pay_total DECIMAL(10, 5) COMMENT '付款金额', " \
    f"pay_type VARCHAR(10) COMMENT '付款类型', " \
    f"payment_channel TINYINT COMMENT '付款通道', " \
    f"payment_scenarios VARCHAR(15) COMMENT '付款描述(无用)', " \
    f"product_count INT COMMENT '本单卖出多少商品', " \
    f"date_ts TIMESTAMP COMMENT '订单时间', " \
    f"INDEX (receivable), INDEX (date_ts)"

# JSON数据采集后，写入MySQL，存储订单详情（带商品信息的）相关的表，表名是：
target_orders_detail_table_name = "orders_detail"
# orders_detail表的建表语句的列信息
target_orders_detail_table_create_cols = \
    f"order_id VARCHAR(255) COMMENT '订单ID', " \
    f"barcode VARCHAR(255) COMMENT '商品条码', " \
    f"name VARCHAR(255) COMMENT '商品名称', " \
    f"count INT COMMENT '本单此商品卖出数量', " \
    f"price_per DECIMAL(10, 5) COMMENT '实际售卖单价', " \
    f"retail_price DECIMAL(10, 5) COMMENT '零售建议价', " \
    f"trade_price DECIMAL(10, 5) COMMENT '贸易价格(进货价)', " \
    f"category_id INT COMMENT '商品类别ID', " \
    f"unit_id INT COMMENT '商品单位ID(包、袋、箱、等)', " \
    f"PRIMARY KEY (order_id, barcode)"

```



#### OrdersModel

在model中新建：retail_ordrs_model.py文件



填入：

```python
# coding:utf8
"""
零售订单模型
负责构建
- 纯订单相关的数据模型（1比1的class模型）
- 订单和商品相关的数据模型（1比多的class模型）
"""
import json
from util import time_util, str_util
from config import project_config as conf


class OrdersModel:
    """构建订单模型（纯订单，不包含商品信息）"""
    def __init__(self, data: str):
        """
        从传入的字符串数据构建订单model
        此Model只包含订单信息，不包含订单详情（商品售卖）
        """
        # 将一行字符串json转换为字典对象
        data = json.loads(data)

        self.discount_rate = data['discountRate']                   # 折扣率
        self.store_shop_no = data['storeShopNo']                    # 店铺店号（无用列）
        self.day_order_seq = data['dayOrderSeq']                    # 本单为当日第几单
        self.store_district = data['storeDistrict']                 # 店铺所在行政区
        self.is_signed = data['isSigned']                           # 是否签约店铺（签约第三方支付体系）
        self.store_province = data['storeProvince']                 # 店铺所在省份
        self.origin = data['origin']                                # 原始信息（无用）
        self.store_gps_longitude = data['storeGPSLongitude']        # 店铺GPS经度
        self.discount = data['discount']                            # 折扣金额
        self.store_id = data['storeID']                             # 店铺ID
        self.product_count = data['productCount']                   # 本单售卖商品数量
        self.operator_name = data['operatorName']                   # 操作员姓名
        self.operator = data['operator']                            # 操作员ID
        self.store_status = data['storeStatus']                     # 店铺状态
        self.store_own_user_tel = data['storeOwnUserTel']           # 店铺店主电话
        self.pay_type = data['payType']                             # 支付类型
        self.discount_type = data['discountType']                   # 折扣类型
        self.store_name = data['storeName']                         # 店铺名称
        self.store_own_user_name = data['storeOwnUserName']         # 店铺店主名称
        self.date_ts = data['dateTS']                               # 订单时间
        self.small_change = data['smallChange']                     # 找零金额
        self.store_gps_name = data['storeGPSName']                  # 店铺GPS名称
        self.erase = data['erase']                                  # 是否抹零
        self.store_gps_address = data['storeGPSAddress']            # 店铺GPS地址
        self.order_id = data['orderID']                             # 订单ID
        self.money_before_whole_discount = data['moneyBeforeWholeDiscount']  # 折扣前金额
        self.store_category = data['storeCategory']                 # 店铺类别
        self.receivable = data['receivable']                        # 应收金额
        self.face_id = data['faceID']                               # 面部识别ID
        self.store_own_user_id = data['storeOwnUserId']             # 店铺店主ID
        self.payment_channel = data['paymentChannel']               # 付款通道
        self.payment_scenarios = data['paymentScenarios']           # 付款情况（无用）
        self.store_address = data['storeAddress']                   # 店铺地址
        self.total_no_discount = data['totalNoDiscount']            # 整体价格（无折扣）
        self.payed_total = data['payedTotal']                       # 已付款金额
        self.store_gps_latitude = data['storeGPSLatitude']          # 店铺GPS纬度
        self.store_create_date_ts = data['storeCreateDateTS']       # 店铺创建时间
        self.store_city = data['storeCity']                         # 店铺所在城市
        self.member_id = data['memberID']                           # 会员ID

    def check_and_transform_area(self):
        """
        检查模型中的省市区三个字段，如果无意义，就转换成未知.
        :return: 无返回值，直接修改类的属性的值
        """
        if str_util.check_null(self.store_province):
            # 表示省份内容无意义
            self.store_province = "未知省份"
        if str_util.check_null(self.store_city):
            # 表示城市内容无意义
            self.store_city = "未知城市"
        if str_util.check_null(self.store_district):
            # 表示行政区内容无意义
            self.store_district = "未知行政区"

    def to_csv(self, sep=","):
        """
        将此模型对象，转换成一条CSV格式的字符串，以参数(sep)传入的符号作为分隔符
        :param sep: 分隔符，默认是逗号
        :return: 字符串，这个字符串就是我们要的CSV格式的字符串
        """
        # CSV：固定分隔符的文件格式，每一条数据，列之间都是有固定的分隔符
        # 比如：
        # 11,张三,北京
        # 13,王五,广州
        # 上述就是CSV，每一行数据用逗号做列之间的分隔
        # csv = f"{self.member_id}{sep}{self.store_city}{sep}{self.store_create_date_ts}  ......................"
        # 将省市区无意义的内容转换成未知省份、未知城市、未知行政区
        self.check_and_transform_area()

        csv_line = \
            f"{self.order_id}{sep}" \
            f"{self.store_id}{sep}" \
            f"{self.store_name}{sep}" \
            f"{self.store_status}{sep}" \
            f"{self.store_own_user_id}{sep}" \
            f"{self.store_own_user_name}{sep}" \
            f"{self.store_own_user_tel}{sep}" \
            f"{self.store_category}{sep}" \
            f"{self.store_address}{sep}" \
            f"{self.store_shop_no}{sep}" \
            f"{self.store_province}{sep}" \
            f"{self.store_city}{sep}" \
            f"{self.store_district}{sep}" \
            f"{self.store_gps_name}{sep}" \
            f"{self.store_gps_address}{sep}" \
            f"{self.store_gps_longitude}{sep}" \
            f"{self.store_gps_latitude}{sep}" \
            f"{self.is_signed}{sep}" \
            f"{self.operator}{sep}" \
            f"{self.operator_name}{sep}" \
            f"{self.face_id}{sep}" \
            f"{self.member_id}{sep}" \
            f"{time_util.ts13_to_date_str(self.store_create_date_ts)}{sep}" \
            f"{self.origin}{sep}" \
            f"{self.day_order_seq}{sep}" \
            f"{self.discount_rate}{sep}" \
            f"{self.discount_type}{sep}" \
            f"{self.discount}{sep}" \
            f"{self.money_before_whole_discount}{sep}" \
            f"{self.receivable}{sep}" \
            f"{self.erase}{sep}" \
            f"{self.small_change}{sep}" \
            f"{self.total_no_discount}{sep}" \
            f"{self.payed_total}{sep}" \
            f"{self.pay_type}{sep}" \
            f"{self.payment_channel}{sep}" \
            f"{self.payment_scenarios}{sep}" \
            f"{self.product_count}{sep}" \
            f"{time_util.ts13_to_date_str(self.date_ts)}"
        return csv_line

    def generate_insert_sql(self):
        """
        将模型转换成一条INSERT SQL语句
        :return: 字符串，记录了INSERT INTO的SQL语句
        """
        sql = f"INSERT IGNORE INTO {conf.target_orders_table_name}(" \
              f"order_id,store_id,store_name,store_status,store_own_user_id," \
              f"store_own_user_name,store_own_user_tel,store_category," \
              f"store_address,store_shop_no,store_province,store_city," \
              f"store_district,store_gps_name,store_gps_address," \
              f"store_gps_longitude,store_gps_latitude,is_signed," \
              f"operator,operator_name,face_id,member_id,store_create_date_ts," \
              f"origin,day_order_seq,discount_rate,discount_type,discount," \
              f"money_before_whole_discount,receivable,erase,small_change," \
              f"total_no_discount,pay_total,pay_type,payment_channel," \
              f"payment_scenarios,product_count,date_ts" \
              f") VALUES(" \
              f"'{self.order_id}', " \
              f"{self.store_id}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_name)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_status)}, " \
              f"{self.store_own_user_id}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_own_user_name)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_own_user_tel)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_category)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_address)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_shop_no)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_province)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_city)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_district)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_gps_name)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_gps_address)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_gps_longitude)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.store_gps_latitude)}, " \
              f"{self.is_signed}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.operator)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.operator_name)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.face_id)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.member_id)}, " \
              f"'{time_util.ts13_to_date_str(self.store_create_date_ts)}', " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.origin)}, " \
              f"{self.day_order_seq}, " \
              f"{self.discount_rate}, " \
              f"{self.discount_type}, " \
              f"{self.discount}, " \
              f"{self.money_before_whole_discount}, " \
              f"{self.receivable}, " \
              f"{self.erase}, " \
              f"{self.small_change}, " \
              f"{self.total_no_discount}, " \
              f"{self.payed_total}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.pay_type)}, " \
              f"{self.payment_channel}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.payment_scenarios)}, " \
              f"{self.product_count}, " \
              f"'{time_util.ts13_to_date_str(self.date_ts)}')"

        return sql



json_str = '{"discountRate": 1, "storeShopNo": "None", "dayOrderSeq": 21, "storeDistrict": "芙蓉区", "isSigned": 0, "storeProvince": "湖南省", "origin": 0, "storeGPSLongitude": "113.00507336854932", "discount": 0, "storeID": 2179, "productCount": 2, "operatorName": "OperatorName", "operator": "NameStr", "storeStatus": "open", "storeOwnUserTel": 12345678910, "payType": "wechat", "discountType": 2, "storeName": "湖南平价特产总汇", "storeOwnUserName": "OwnUserNameStr", "dateTS": 1542458768000, "smallChange": 0, "storeGPSName": "None", "erase": 0, "product": [{"count": 3, "name": "伊利优酸乳草莓味250ml", "unitID": 3, "barcode": "6907992100012", "pricePer": 2.5, "retailPrice": 2.5, "tradePrice": 1.8, "categoryID": 11}, {"count": 3, "name": "巧克力面包", "unitID": 4, "barcode": "6971518660038", "pricePer": 3, "retailPrice": 3, "tradePrice": 1.5, "categoryID": 1}], "storeGPSAddress": "None", "orderID": "154245876623021796472", "moneyBeforeWholeDiscount": 16.5, "storeCategory": "normal", "receivable": 16.5, "faceID": "", "storeOwnUserId": 2127, "paymentChannel": 0, "paymentScenarios": "PASV", "storeAddress": "StoreAddress", "totalNoDiscount": 16.5, "payedTotal": 16.5, "storeGPSLatitude": "28.194364121754337", "storeCreateDateTS": 1541746839000, "storeCity": "长沙市", "memberID": "0"}'
model = OrdersModel(json_str)
print(model.generate_insert_sql())

```



#### OrdersDetailModel

订单详情模型，包含订单ID + 商品信息



在`retail_orders_model.py`下面，继续写：

```python
class OrdersDetailModel:
    """订单详情模型，仅包含订单ID+商品信息数据"""

    def __init__(self, data):
        """定义模型中的属性（成员变量），用来存储数据"""
        # 将传入的JSON字符串，转换成Python字典
        data_dict = json.loads(data)
        self.order_id = data_dict['orderID']        # 订单ID
        # 记录当前订单内，售卖的全部商品信息，每一条数据是一个SingleProductSoldModel的对象
        self.products_detail = []
        # 从字典中取出商品信息的list
        order_products_list = data_dict["product"]
        # for循环商品信息list，将每一个商品信息转换成SingleProductSoldModel对象，存入products_detail中
        for single_product in order_products_list:
            self.products_detail.append(SingleProductSoldModel(self.order_id, single_product))

    def generate_insert_sql(self):
        """
        生成插入MySQL的INSERT SQL语句
        可以完成一次性插入多条数据
        """
        sql = f"INSERT IGNORE INTO {conf.target_orders_detail_table_name}(" \
              f"order_id,barcode,name,count,price_per,retail_price,trade_price,category_id,unit_id) VALUES"
        # 当前这个SQL字符串是半成品，类似于：`INSERT IGNORE INTO table(id, name) VALUES`
        for model in self.products_detail:
            # model：SingleProductSoldModel的一个对象
            sql += "("
            sql += f"'{model.order_id}', " \
                   f"{str_util.check_str_null_and_transform_to_sql_null(model.barcode)}, " \
                   f"{str_util.check_str_null_and_transform_to_sql_null(model.name)}, " \
                   f"{model.count}, " \
                   f"{model.price_per}, " \
                   f"{model.retail_price}, " \
                   f"{model.trade_price}, " \
                   f"{model.category_id}, " \
                   f"{model.unit_id}"
            sql += "), "

        # 去除SQL结尾的逗号
        sql = sql[:-2]  # 为什么是-2，因为逗号后面还有一个空格

        return sql

    def to_csv(self, sep=","):
        csv_line = ""
        for model in self.products_detail:
            csv_line += model.to_csv()
            csv_line += "\n"
        return csv_line


class SingleProductSoldModel:
    """订单内售卖的单类商品信息"""
    def __init__(self, order_id, product_detail_dict):
        self.order_id = order_id                                    # 订单ID
        self.name = product_detail_dict["name"]                     # 商品名称
        self.count = product_detail_dict["count"]                   # 商品售卖数量
        self.unit_id = product_detail_dict["unitID"]                # 单位ID
        self.barcode = product_detail_dict["barcode"]               # 商品的条码
        self.price_per = product_detail_dict["pricePer"]            # 商品卖出的单价
        self.retail_price = product_detail_dict["retailPrice"]      # 商品建议零售价
        self.trade_price = product_detail_dict["tradePrice"]        # 商品建议成本价
        self.category_id = product_detail_dict["categoryID"]        # 商品类别ID

    def to_csv(self, sep=","):
        """生成一条csv数据，分隔符默认逗号"""
        csv_line = \
            f"{self.order_id}{sep}" \
            f"{self.barcode}{sep}" \
            f"{self.name}{sep}" \
            f"{self.count}{sep}" \
            f"{self.price_per}{sep}" \
            f"{self.retail_price}{sep}" \
            f"{self.trade_price}{sep}" \
            f"{self.category_id}{sep}" \
            f"{self.unit_id}"

        return csv_line

```





### 填充主逻辑代码(写出CSV）：json_service.py

在json_service.py中编写代码，截止到目前为止，主逻辑代码的全部内容如下：

```python
# coding:utf8
"""
采集JSON数据（订单数据）到
MySQL和CSV的功能代码逻辑
"""
import sys

from util.logging_util import init_logger
from util import file_util as fu
from config import project_config as conf
from util.mysql_util import MySQLUtil, get_processed_files
from model.retail_orders_model import OrdersModel, OrdersDetailModel, SingleProductSoldModel

# TODO: 步骤1：找出哪些文件可以供我们处理

# 首先，获取到logger对象，用于后续输出日志用
logger = init_logger()
logger.info("读取JSON数据处理，程序开始执行了......")

# 判断，JSON数据所在的文件夹下面有哪些文件可以供我们读取
files = fu.get_dir_files_list(conf.json_data_root_path)
logger.info(f"判断json的文件夹，发现有如下文件：{files}")

# 判断，这些文件哪些是可以处理的，哪些是已经处理过的
db_util = MySQLUtil()
# 获取哪些文件是已经处理过的
processed_files = get_processed_files(db_util)
logger.info(f"查询MySQL，找到有如下文件已经被处理过了：{processed_files}")

# 对比files和processed_files，找出没有被处理的文件供我们使用
# 调用工具方法，对比它们
need_to_process_files = fu.get_new_by_compare_lists(processed_files, files)
logger.info(f"经过对比mysql元数据库，找出如下文件供我们处理：{need_to_process_files}")

# TODO: 步骤2：开始处理文件
# 依次处理文件
count = 0
for file in need_to_process_files:

    # 存储所有的订单模型对象
    order_model_list = []
    # 存储所有的订单详情模型对象
    order_detail_model_list = []

    # 通过open读取文件
    for line in open(file, "r", encoding="UTF-8"):
        # line 就是每一行数据，先将里面的 回车符先删除
        line = line.replace("\n", "")
        order_model = OrdersModel(line)
        order_detail_model = OrdersDetailModel(line)
        order_model_list.append(order_model)
        order_detail_model_list.append(order_detail_model)

    # 过滤数据
    # 数据中，有一个字段，receivable表示本订单卖了多少钱。
    # 数据中有许多的测试数据，receivable的金额非常大，我们做一个简单的判断，大于10000的这个数据我们就不要了
    reserved_models = []
    for model in order_model_list:
        if model.receivable <= 10000:
            reserved_models.append(model)

    # 写出CSV和MySQL
    # 用来写出订单模型的文件对象
    order_csv_write_f = open(
        conf.retail_output_csv_root_path + conf.retail_orders_output_csv_file_name, "w", encoding="UTF-8"
    )
    # 用来写出订单详情模型的文件对象
    order_detail_csv_write_f = open(
        conf.retail_output_csv_root_path + conf.retail_orders_detail_output_csv_file_name, "w", encoding="UTF-8"
    )

    # 先处理订单模型
    for model in reserved_models:
        csv_line = model.to_csv()
        # 写出一行csv数据
        order_csv_write_f.write(csv_line)
        # 再写出一个换行符
        order_csv_write_f.write("\n")
    order_csv_write_f.close()

    # 接着处理订单详情模型
    for model in order_detail_model_list:
        # 每一个model就是一个OrdersDetailModel对象
        for single_product_model in model.products_detail:
            csv_line = single_product_model.to_csv()
            order_detail_csv_write_f.write(csv_line)
            order_detail_csv_write_f.write("\n")

    order_detail_csv_write_f.close()


logger.info(f"完成了CSV备份文件的写出，写出到了：{conf.retail_output_csv_root_path}")

```



截止到目前，已经完成了CSV备份数据的写出了。







### 填充主业务逻辑代码（完全完成）：json_service.py

```python
# coding:utf8
"""
采集JSON数据（订单数据）到
MySQL和CSV的功能代码逻辑
"""
import sys
import time

from util.logging_util import init_logger
from util import file_util as fu
from config import project_config as conf
from util.mysql_util import MySQLUtil, get_processed_files
from model.retail_orders_model import OrdersModel, OrdersDetailModel, SingleProductSoldModel

# TODO: 步骤1：找出哪些文件可以供我们处理
# 首先，获取到logger对象，用于后续输出日志用
logger = init_logger()
logger.info("读取JSON数据处理，程序开始执行了......")

# 判断，JSON数据所在的文件夹下面有哪些文件可以供我们读取
files = fu.get_dir_files_list(conf.json_data_root_path)
logger.info(f"判断json的文件夹，发现有如下文件：{files}")

# 获取元数据连接的db_util对象
metadata_db_util = MySQLUtil()
# 获取目标数据库连接的db_util对象
target_db_util = MySQLUtil(conf.target_host, conf.target_user, conf.target_password,
                           conf.target_port)

# 获取哪些文件是已经处理过的
processed_files = get_processed_files(metadata_db_util)
logger.info(f"查询MySQL，找到有如下文件已经被处理过了：{processed_files}")

# 对比files和processed_files，找出没有被处理的文件供我们使用
# 调用工具方法，对比它们
need_to_process_files = fu.get_new_by_compare_lists(processed_files, files)
logger.info(f"经过对比mysql元数据库，找出如下文件供我们处理：{need_to_process_files}")

# TODO: 步骤2：开始处理文件
# 依次处理文件
# 全局计数器，记录本次执行总共处理了多少条数据
global_count = 0
# 被处理的文件信息记录
processed_files_record_dict = {}
for file in need_to_process_files:
    # 记录此文件被处理了多少条的计数器
    file_processed_lines_count = 0

    # 存储所有的订单模型对象
    order_model_list = []
    # 存储所有的订单详情模型对象
    order_detail_model_list = []

    # 通过open读取文件
    for line in open(file, "r", encoding="UTF-8"):
        global_count += 1
        file_processed_lines_count += 1
        # line 就是每一行数据，先将里面的 回车符先删除
        line = line.replace("\n", "")
        order_model = OrdersModel(line)
        order_detail_model = OrdersDetailModel(line)
        order_model_list.append(order_model)
        order_detail_model_list.append(order_detail_model)

    # 过滤数据
    # 数据中，有一个字段，receivable表示本订单卖了多少钱。
    # 数据中有许多的测试数据，receivable的金额非常大，我们做一个简单的判断，大于10000的这个数据我们就不要了
    reserved_models = []
    for model in order_model_list:
        if model.receivable <= 10000:
            reserved_models.append(model)

    # 写出CSV和MySQL
    # 先写出CSV
    # 用来写出订单模型的文件对象
    order_csv_write_f = open(
        conf.retail_output_csv_root_path + conf.retail_orders_output_csv_file_name, "a", encoding="UTF-8"
    )
    # 用来写出订单详情模型的文件对象
    order_detail_csv_write_f = open(
        conf.retail_output_csv_root_path + conf.retail_orders_detail_output_csv_file_name, "a", encoding="UTF-8"
    )

    # 先处理订单模型
    for model in reserved_models:
        csv_line = model.to_csv()
        # 写出一行csv数据
        order_csv_write_f.write(csv_line)
        # 再写出一个换行符
        order_csv_write_f.write("\n")
    order_csv_write_f.close()

    # 接着处理订单详情模型
    for model in order_detail_model_list:
        # 每一个model就是一个OrdersDetailModel对象
        for single_product_model in model.products_detail:
            csv_line = single_product_model.to_csv()
            order_detail_csv_write_f.write(csv_line)
            order_detail_csv_write_f.write("\n")

    order_detail_csv_write_f.close()

    # 将数据写出到MySQL中
    # 先判断被写入的MySQL的表是否存在，如果不存在，先创建它们
    # 先判断订单表
    target_db_util.check_table_exists_and_create(
        conf.target_db_name,
        conf.target_orders_table_name,
        conf.target_orders_table_create_cols
    )
    # 在判断订单详情表
    target_db_util.check_table_exists_and_create(
        conf.target_db_name,
        conf.target_orders_detail_table_name,
        conf.target_orders_detail_table_create_cols
    )

    # 先将订单数据，写入订单表
    for model in reserved_models:
        # 每一个model就是一个 OrdersModel的订单模型的对象
        insert_sql = model.generate_insert_sql()
        # 选择被操作数据库
        target_db_util.select_db(conf.target_db_name)
        # 选择target_db_util，使用execute 方法插入数据
        target_db_util.execute_without_autocommit(insert_sql)

    # 写入订单详情数据
    for model in order_detail_model_list:
        # 每一个Model 就是一个OrderDetailModel
        insert_sql = model.generate_insert_sql()
        # 选择被操作数据库
        target_db_util.select_db(conf.target_db_name)
        # 选择target_db_util，使用execute 方法插入数据
        target_db_util.execute_without_autocommit(insert_sql)

    processed_files_record_dict[file] = file_processed_lines_count

    # 记录元数据


# 一次批量将缓冲区内的暂存insert sql语句全部一次性提交到mysql
target_db_util.conn.commit()

logger.info(f"完成了CSV备份文件的写出，写出到了：{conf.retail_output_csv_root_path}")
logger.info(f"完成了向MySQL数据库中插入数据的操作。"
            f"共处理了：{global_count}条数据")

# 将已经处理完成的文件， 记录到MySQL中
for file_name in processed_files_record_dict.keys():
    # 文件被处理的行
    file_processed_lines = processed_files_record_dict[file_name]
    # 文件的名，就是file_name本身

    # 组装INSERT INTO
    insert_sql = f"INSERT INTO {conf.metadata_file_monitor_table_name}(file_name, process_lines) " \
                 f"VALUES('{file_name}', {file_processed_lines})"
    metadata_db_util.execute(insert_sql)

# 最后将所有的数据库链接关闭
metadata_db_util.close_conn()
target_db_util.close_conn()
logger.info("读取JSON数据向MySQL插入以及写出CSV备份，程序执行完成......")

```





> 截止到这里，主业务逻辑代码全部完成。
>
> 执行流程：
>
> 1. 从指定文件夹下读取有哪些文件
> 2. 从MySQL元数据中读取哪些文件被处理了
> 3. 对比MySQL的记录，找出哪些文件是没有被处理的
> 4. 对没有被处理的文件进行for循环，得到每一行数据
> 5. 每一行数据被转换成对应的2个模型
>    1. OrderModel订单模型
>    2. OrderDetailModel订单详情模型
> 6. 将两个模型放入list中保存
> 7. 将订单模型进行过滤，价格大于10000的不要
> 8. for循环两个list，将数据写出CSV
> 9. for循环2个list，将数据写入MySQL
> 10. 将处理好的文件，记录到MySQL元数据库中
> 11. 关闭链接程序结束





# 需求2：从MySQL中采集商品信息

我们给同学们提供了一份数据文件，在：`课程资料`里面有一个`barcode条码库.sql`

我们的被采集的数据，都在这里面。



## 前期准备

将被采集的数据导入到MySQL的数据源库中



将脚本拖入：`DataGrip`中

右键选择脚本文件名称，点击运行：

![image-20220519114543742](C:\Users\caoyu\AppData\Roaming\Typora\typora-user-images\image-20220519114543742.png)



![image-20220519114631623](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519114631.png)



导入完成后，即可在`source_data`数据库中见到这个表：

![image-20220519114820826](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519114820.png)

简化版的数据有5000多条，够用了。

完整版的有上百万条，看电脑，建议简化版即可。





我们现在要做的，就是执行MySQL的数据采集，将数据从`source_data`库，采集到`retail`库。

> 课程中为了学习，不是真正的环境场景
>
> 我们是用一个MySQL 全部完成这个操作
>
> 如果是在真实的企业场景下，source_data库 和 retail库，很可能不在同一个MySQL内

![image-20220519115022396](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519115022.png)



sys_barcode表的结构如下：

```sql
CREATE TABLE IF NOT EXISTS sys_barcode (
  `code` varchar(50) PRIMARY KEY COMMENT '商品条码',
  `name` varchar(200) DEFAULT '' COMMENT '商品名称',
  `spec` varchar(200) DEFAULT '' COMMENT '商品规格',
  `trademark` varchar(100) DEFAULT '' COMMENT '商品商标',
  `addr` varchar(200) DEFAULT '' COMMENT '商品产地',
  `units` varchar(50) DEFAULT '' COMMENT '商品单位(个、杯、箱、等)',
  `factory_name` varchar(200) DEFAULT '' COMMENT '生产厂家',
  `trade_price` DECIMAL(50, 5) DEFAULT 0.0 COMMENT '贸易价格(指导进价)',
  `retail_price` DECIMAL(50, 5) DEFAULT 0.0 COMMENT '零售价格(建议卖价)',
  `update_at` timestamp NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  `wholeunit` varchar(50) DEFAULT NULL COMMENT '大包装单位',
  `wholenum` int(11) DEFAULT NULL COMMENT '大包装内装数量',
  `img` varchar(500) DEFAULT NULL COMMENT '商品图片',
  `src` varchar(20) DEFAULT NULL COMMENT '源信息', 
  INDEX (update_at)
) DEFAULT CHARSET=utf8;
```



## 采集的要求

我们的采集要做到`增量更新`采集



假设当前被采集的表有如下数据:

| barcode | name     | price | unit |
| ------- | -------- | ----- | ---- |
| 6900001 | 娃哈哈   | 1     | 瓶   |
| 6900002 | 农夫山泉 | 2     | 瓶   |
| 6900003 | 可口可乐 | 3     | 瓶   |
| 6900004 | 雪碧     | 1     | 瓶   |

这4条数据需要被采集到目标表中。



当这一次采集完成后，被采集的表数据发生了更新，新增了一行以及有一条数据的商品名被修改了

| barcode | name         | price | unit |
| ------- | ------------ | ----- | ---- |
| 6900001 | 娃哈哈       | 1     | 瓶   |
| 6900002 | 农夫山泉     | 2     | 瓶   |
| 6900003 | 可口可乐     | 3     | 瓶   |
| 6900004 | **无糖雪碧** | 1     | 瓶   |
| 6900005 | **芬达**     | 1     | 瓶   |

如上表，第四行 名字发生了更改，第五行是新增的。



我们下一次执行采集的时候，需要做到 只采集4、5两行数据，覆盖到目标表中即可

第123行数据不理会即可。



### 如何实现

需要基于被采集的表中的一个字段：`update_at`

![image-20220519144344038](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519144344.png)

如图，表中有这个字段，意思是：

- 记录数据被修改的时间（更新）
- 记录数据被插入数据库的时间（新增）

这个字段凡是新数据以及更新的数据，这个字段都会记录当时的时间。



#### 思路

- 查询（采集）数据的时候，SELECT SQL语句按照`update_at`进行排序，按照`升序`排序
- 当采集完成后，将当前批次最大的时间记录在：MySQL的`元数据库中`
- 下一次采集的时候，从MySQL的元数据库中，查询出来`上一次采集的时间`
- SQL的SELECT语句的WHERE条件设置为： `update_at >= 上一次采集时间` 即可







## 代码开发

创建主业务逻辑类： 在工程的根目录下，创建`mysql_service.py`



思考：我们会涉及到几个数据库？答案是3：

1. 元数据库：用来记录上一次采集的时间
2. 数据源库：提供被采集数据的
3. 目的地库：用来写入数据的





### 新增工具方法

在str_util.py中新增：

```python
def check_number_null_and_transform_to_sql_null(data):
    """
    检查数字，如果是空内容，就返回SQL意义上的NULL（插入的SQL语句中会插入真正的NULL）
    如果是有意义的内容，返回 内容本身
    :param data:
    :return:
    """
    if data and not check_null(str(data)):
        # and 两个是True才能进来if
        # data如果不是None，而是有内容，那么就能进来
        # 同时not check_null(str(data))是True 才能进来
        # 也就是check_null(str(data))是 False， 表示有意义
        # 总结：必须满足data有内容（不是None）同时满足data的内容有意义才会进入if
        # 说明是正常数据
        return data
    else:
        # 这个数据有问题
        return "NULL"       # return SQL意义上的NULL


def clean_str(data):
    if check_null(data):
        # 如果是无意义的内容，比如字符串None、字符串Null 等，这些不影响插入操作不理会
        return data

    # 如果是有意义的内容，需要处理，比如： 可口可乐\    内容中自带斜杠导致程序出错
    # 乱七八糟的符号，我们要处理掉
    data = data.replace("'", "")
    data = data.replace('"', "")
    data = data.replace("\\", "")
    data = data.replace(";", "")
    data = data.replace(",", "")
    data = data.replace("@", "")

    return data
```



### 新增模型

在model中新建：`barcode_model.py`

```python
# coding:utf8
"""
条码商品信息模型
"""
from config import project_config as conf
from util import str_util


class BarcodeModel:
    def __init__(self, code=None, name=None, spec=None, trademark=None,
                 addr=None, units=None, factory_name=None, trade_price=None,
                 retail_price=None, update_at=None, wholeunit=None,
                 wholenum=None, img=None, src=None):
        self.code = code
        self.name = str_util.clean_str(name)
        self.spec = str_util.clean_str(spec)
        self.trademark = str_util.clean_str(trademark)
        self.addr = str_util.clean_str(addr)
        self.units = str_util.clean_str(units)
        self.factory_name = str_util.clean_str(factory_name)
        self.trade_price = trade_price
        self.retail_price = retail_price
        self.update_at = update_at
        self.wholeunit = str_util.clean_str(wholeunit)
        self.wholenum = wholenum
        self.img = img
        self.src = src

    def generate_insert_sql(self):
        """生成SQL的插入语句"""
        sql = f"REPLACE INTO {conf.target_barcode_table_name}(" \
              f"code,name,spec,trademark,addr,units,factory_name,trade_price," \
              f"retail_price,update_at,wholeunit,wholenum,img,src) VALUES(" \
              f"'{self.code}', " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.name)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.spec)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.trademark)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.addr)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.units)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.factory_name)}, " \
              f"{str_util.check_number_null_and_transform_to_sql_null(self.trade_price)}, " \
              f"{str_util.check_number_null_and_transform_to_sql_null(self.retail_price)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.update_at)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.wholeunit)}, " \
              f"{str_util.check_number_null_and_transform_to_sql_null(self.wholenum)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.img)}, " \
              f"{str_util.check_str_null_and_transform_to_sql_null(self.src)}" \
              f")"

        return sql

    def to_csv(self, sep=","):
        csv_line = \
            f"{self.code}{sep}" \
            f"{self.name}{sep}" \
            f"{self.spec}{sep}" \
            f"{self.trademark}{sep}" \
            f"{self.addr}{sep}" \
            f"{self.units}{sep}" \
            f"{self.factory_name}{sep}" \
            f"{self.trade_price}{sep}" \
            f"{self.retail_price}{sep}" \
            f"{self.update_at}{sep}" \
            f"{self.wholeunit}{sep}" \
            f"{self.wholenum}{sep}" \
            f"{self.img}{sep}" \
            f"{self.src}"

        return csv_line

```





### 编写主业务逻辑代码

在工程根目录，新建：`mysql_service.py`

```python
# coding:utf8
"""
采集MySQL中的条码库（商品信息）数据
采集到目标的MySQL中
"""
import sys
from model.barcode_model import BarcodeModel
from util.logging_util import init_logger
from util.mysql_util import MySQLUtil
from config import project_config as conf

# 构建日志对象
logger = init_logger()
logger.info("采集MySQL数据的程序启动......")

# TODO：步骤1：构建数据库util对象

# 构建MySQL的数据库Util工具
# 这个需求设计到：元数据、数据源、目的地 三个数据库，我们构建3个db_util工具
# 构建元数据库的util对象
metadata_db_util = MySQLUtil()
# 构建数据源数据库的util对象
source_db_util = MySQLUtil(
    host=conf.source_host,
    user=conf.source_user,
    password=conf.source_password,
    port=conf.source_port
)
# 构建目的地数据库的util对象
target_db_util = MySQLUtil(
    host=conf.target_host,
    user=conf.target_user,
    password=conf.target_password,
    port=conf.target_port
)

# TODO：步骤2：从数据源中读取数据
# 先判断，供我们读取数据的表是否存在？如果不存在?***程序退出***.
if not source_db_util.check_table_exists(conf.source_db_name, conf.source_barcode_data_table_name):
    # 进入if 表示表不存在
    logger.error(f"数据源库：{conf.source_db_name}中不存在数据源表：{conf.source_barcode_data_table_name}，"
                 f"无法采集，程序退出，请开展社交找人要去。。。。。。")
    sys.exit(1)     # sys.exit()表示python程序停止运行，传入0表示是正常的停止，传入非0的数字表示程序是异常停止

# 接着判断，目的地的MySQL库中，是否有我们要写入数据的表？如果没有？***那么就新建***
target_db_util.check_table_exists_and_create(
    conf.target_db_name,
    conf.target_barcode_table_name,
    conf.target_barcode_table_create_cols
)

# 开始查询的流程了
# 注意：我们每一次查询，都要去判断一下update_at，来决定本次查询从哪个时间开始
# 上一次查询的最大时间，会记录到mysql中，我们在查询数据之前，需要先从mysql中查询出来
# 上一次查询的时间。
# 选择元数据库
metadata_db_util.select_db(conf.metadata_db_name)

# 从元数据表中查询上一次的记录，先判断，这元数据表是否存在，不存在？要创建
# 定义一个变量，记录上一次查询的时间
last_update_time = None
if not metadata_db_util.check_table_exists(conf.metadata_db_name, conf.metadata_barcode_table_name):
    # 进入if，表示表不存在 需要创建表
    metadata_db_util.check_table_exists_and_create(
        conf.metadata_db_name,
        conf.metadata_barcode_table_name,
        conf.metadata_barcode_table_create_cols
    )
else:
    # 进入else表示表存在，需要从MySQL中查询出上一次查询的时间
    query_sql = f"SELECT time_record FROM {conf.metadata_barcode_table_name} ORDER BY time_record DESC LIMIT 1"
    result = metadata_db_util.query(query_sql)
    # 表存在 不代表里面有数据
    if len(result) != 0:

        # 代表有表，并且查询到数据了
        last_update_time = str(result[0][0])

# 准备SQL查询的语句
if last_update_time:
    # 表示last_update_time不是None
    sql = f"SELECT * FROM {conf.source_barcode_data_table_name} WHERE updateAt >= '{last_update_time}' " \
          f"ORDER BY updateAt"
else:
    # 表示last_update_time 是None
    sql = f"SELECT * FROM {conf.source_barcode_data_table_name} ORDER BY updateAt"
# 开始执行SQL查询
# 选择数据库
source_db_util.select_db(conf.source_db_name)
# 执行SQL查询
result = source_db_util.query(sql)
# pymysql的查询结果是： ( (结果的某行),  (结果的某行), (结果的某行), (结果的某行)......  )
# (结果的某行) 是：元组类型，里面存储的是查询结果的列，比如：(1, '张三', 11)......
# TODO：步骤3：开始准备模型了构建INSERT INTO 插入语句
# 构建模型

barcode_models = []
for single_line_result in result:
    # single_line_result是元组，存储了一条结果的列
    # 查询是SELECT * 查询，结果中的第一个列 第二个列 第N个列是谁，和数据库的表结构顺序一致
    code = single_line_result[0]
    name = single_line_result[1]
    spec = single_line_result[2]
    trademark = single_line_result[3]
    addr = single_line_result[4]
    units = single_line_result[5]
    factory_name = single_line_result[6]
    trade_price = single_line_result[7]
    retail_price = single_line_result[8]
    update_at = str(single_line_result[9])  # single_line_result[9]是读取的updateAt时间，类型是datetime，转换成字符串
    wholeunit = single_line_result[10]
    wholenum = single_line_result[11]
    img = single_line_result[12]
    src = single_line_result[13]

    # 构建BarcodeModel
    model = BarcodeModel(
        code=code,
        name=name,
        spec=spec,
        trademark=trademark,
        addr=addr,
        units=units,
        factory_name=factory_name,
        trade_price=trade_price,
        retail_price=retail_price,
        update_at=update_at,
        wholeunit=wholeunit,
        wholenum=wholenum,
        img=img,
        src=src
    )
    barcode_models.append(model)

# 至此，我们有了barcode_models list，里面存放了一堆barcode model对象

# TODO：步骤4：开始写入

# 切换数据库
target_db_util.select_db(conf.target_db_name)
max_last_update_time = "2000-01-01 00:00:00"
# for循环 挨个执行SQL插入
count = 0
for model in barcode_models:
    # 取出```本```条数据的处理时间， current：当前的
    current_data_time = model.update_at
    # 判断当前这个时间是否大于记录的max_last_update_time
    if current_data_time > max_last_update_time:
        max_last_update_time = current_data_time

    # 生成插入SQL语句
    insert_sql = model.generate_insert_sql()
    # 执行插入，使用execute_without_autocommit 配合autocommit为False应用批量提交提高性能
    target_db_util.execute_without_autocommit(insert_sql)

    count += 1
    if count % 1000 == 0:
        # 每隔1000条提交一次
        target_db_util.conn.commit()
        logger.info(f"从数据源：{conf.source_db_name}库，读取表：{conf.source_barcode_data_table_name}，"
                    f"当前写入目标表：{conf.target_barcode_table_name}数据有：{count}行")

# 一次性批量提交
target_db_util.conn.commit()
logger.info(f"从数据源：{conf.source_db_name}库，读取表：{conf.source_barcode_data_table_name}，"
            f"当前写入目标表：{conf.target_barcode_table_name}完成，最终写入：{count}行")

# 写出CSV
barcode_csv_write_f = open(
    conf.barcode_output_csv_root_path + conf.barcode_orders_output_csv_file_name,
    "a",
    encoding="UTF-8"
)

count = 0
for model in barcode_models:
    csv_line = model.to_csv()
    barcode_csv_write_f.write(csv_line)
    barcode_csv_write_f.write("\n")
    count += 1

    if count % 1000 == 0:
        # 每隔1000条，清空缓冲区，将内容写入文件
        barcode_csv_write_f.flush()
        logger.info(f"从数据源：{conf.source_db_name}库，读取表：{conf.source_barcode_data_table_name}，"
                    f"写出CSV到：{barcode_csv_write_f.name}， 当前写出：{count}行。")

barcode_csv_write_f.close()
logger.info(f"从数据源：{conf.source_db_name}库，读取表：{conf.source_barcode_data_table_name}，"
            f"写出CSV到：{barcode_csv_write_f.name} 完成， 最终写出：{count}行。")


# TODO:步骤5：记录MySQL元数据
metadata_db_util.select_db(conf.metadata_db_name)
# 准备sql
sql = f"INSERT INTO {conf.metadata_barcode_table_name}(" \
      f"time_record, gather_line_count) VALUES(" \
      f"'{max_last_update_time}', " \
      f"{count}" \
      f")"
# 执行插入
metadata_db_util.execute(sql)

# 关闭全部数据库连接
metadata_db_util.close_conn()
source_db_util.close_conn()
target_db_util.close_conn()

logger.info("读取MySQL数据，写入目标MySQL和CSV程序执行完成。。。。。。")

```





# 需求3：采集后台日志数据



## 数据模拟器（数据来源）

后台日志数据，可以通过课程中提供的一个python文件，来生成需要的模拟数据。

在工程的根目录新建一个文件夹：`simulator`

在里面新建一个python文件：`backend_logs_simulator.py`



```python
# coding:utf8
"""
后端服务写出log日志的模拟数据生成器
"""
import datetime
import random
import time

single_log_lines = 1024  # 一个logs文件生成多少行数据
generate_files = 5  # 一次运行生成多少个文件

output_path = "e:/pyetl-data-logs/backend_logs/"
log_level_array = ['WARN', 'WARN', 'WARN', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO',
                   'INFO', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO', 'INFO',
                   'ERROR']

backend_files_name = ['barcode_service.py', 'barcode_service.py', 'barcode_service.py',
                      'orders_service.py', 'orders_service.py', 'orders_service.py', 'orders_service.py',
                      'orders_service.py', 'orders_service.py',
                      'shop_manager.py', 'shop_manager.py',
                      'user_manager.py', 'user_manager.py', 'user_manager.py',
                      'goods_manager.py', 'goods_manager.py', 'goods_manager.py', 'goods_manager.py',
                      'goods_manager.py', 'goods_manager.py',
                      'base_network.py', 'base_network.py',
                      'event.py', 'event.py', 'event.py', 'event.py', 'event.py', 'event.py', 'event.py']

visitor_areas = {
    '北京市': ['海淀区', '大兴区', '丰台区', '朝阳区', '昌平区', '海淀区', '怀柔区'],
    '上海市': ['静安区', '黄浦区', '徐汇区', '普陀区', '杨浦区', '宝山区', '浦东新区', '浦东新区'],
    '重庆市': ['万州区', '万州区', '涪陵区', '渝中区', '沙坪坝区', '九龙坡区', '南岸区'],
    '江苏省': ['南京市', '南京市', '南京市', '苏州市', '苏州市', '无锡市', '常州市', '宿迁市', '张家港市'],
    '安徽省': ['阜阳市', '阜阳市', '六安市', '合肥市', '合肥市', '合肥市', '池州市', '铜陵市', '芜湖市'],
    '山东省': ['济南市', '济南市', '青岛市', '青岛市', '青岛市', '菏泽市'],
    '湖北省': ['武汉市', '武汉市', '武汉市', '十堰市', '荆州市', '恩施土家族苗族自治州'],
    '广东省': ['广州市', '广州市', '广州市', '深圳市', '深圳市', '深圳市', '珠海市'],
    '天津市': ['和平区', '河东区', '河西区', '武清区', '宝坻区'],
    '湖南省': ['长沙市', '长沙市', '长沙市', '长沙市', '长沙市', '长沙市', '长沙市', '株洲市', '张家界市', '常德市', '益阳市'],
    '浙江省': ['杭州市', '杭州市', '湖州市', '绍兴市', '舟山市', '金华市', '嘉兴市', '丽水市']
}
visitor_province = ['北京市', '上海市', '重庆市', '江苏省', '安徽省', '山东省', '湖北省', '广东省', '天津市', '湖南省', '浙江省']

response_flag = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]
response_for_error_flag = [1, 1, 1, 1, 1, 0]

for j in range(0, generate_files):
    write_file_path = f'{output_path}{datetime.datetime.now().strftime("%Y-%m-%d-%H-%M-%S")}.log'
    with open(write_file_path, 'w', encoding="UTF-8") as f:
        for i in range(single_log_lines):
            date_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")
            log_level = log_level_array[random.randint(0, len(log_level_array) - 1)]
            file_name = backend_files_name[random.randint(0, len(backend_files_name) - 1)]
            if not log_level == "ERROR":
                if response_flag[random.randint(0, len(response_flag) - 1)] == 1:
                    response_time = random.randint(0, 1000)
                else:
                    response_time = random.randint(1000, 9999)
            else:
                if response_for_error_flag[random.randint(0, len(response_for_error_flag) - 1)] == 1:
                    response_time = random.randint(0, 1000)
                else:
                    response_time = random.randint(1000, 9999)

            province = visitor_province[random.randint(0, len(visitor_province) - 1)]
            city = visitor_areas[province][random.randint(0, len(visitor_areas[province]) - 1)]

            log_str = f"{date_str}\t[{log_level}]\t{file_name}\t响应时间:{response_time}ms\t{province}\t{city}\t" \
                      f"这里是日志信息......"

            f.write(log_str)
            f.write("\n")
    print(f"本次写出第: {j + 1}个文件完成, 文件为: {write_file_path}, 行数:{single_log_lines}")
    time.sleep(1)

```



> 注意，模拟器中第12行，output_path变量
>
> 记录生成的数据写到哪个文件夹下。
>
> 用的时候，先改一下设定为你电脑上的文件夹即可
>
> 然后右键直接运行即可



## 数据结构

示例数据：

```shell
2022-05-20 09:14:11.704033	[INFO]	base_network.py	响应时间:326ms	浙江省	绍兴市	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	event.py	响应时间:36ms	湖北省	荆州市	这里是日志信息......
2022-05-20 09:14:11.704033	[ERROR]	event.py	响应时间:815ms	重庆市	九龙坡区	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	event.py	响应时间:570ms	安徽省	六安市	这里是日志信息......
2022-05-20 09:14:11.704033	[WARN]	base_network.py	响应时间:766ms	湖南省	长沙市	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	goods_manager.py	响应时间:132ms	江苏省	宿迁市	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	event.py	响应时间:629ms	湖南省	长沙市	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	goods_manager.py	响应时间:254ms	上海市	浦东新区	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	goods_manager.py	响应时间:583ms	山东省	青岛市	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	goods_manager.py	响应时间:702ms	天津市	宝坻区	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	orders_service.py	响应时间:117ms	安徽省	合肥市	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	base_network.py	响应时间:442ms	天津市	河西区	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	goods_manager.py	响应时间:358ms	北京市	大兴区	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	orders_service.py	响应时间:440ms	重庆市	渝中区	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	barcode_service.py	响应时间:809ms	安徽省	阜阳市	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	base_network.py	响应时间:720ms	浙江省	金华市	这里是日志信息......
2022-05-20 09:14:11.704033	[INFO]	goods_manager.py	响应时间:874ms	北京市	海淀区	这里是日志信息......
```

它的结构就是：

| 日志时间 | 日志级别 | 代码模块 | 接口响应时间 | 调用者省份 | 调用者城市 | 日志信息 |
| -------- | -------- | -------- | ------------ | ---------- | ---------- | -------- |





## 进入代码的开发

### 粗略的代码流程

1. 读取文件夹中有哪些文件
2. 从MySQL的元数据库中查询哪些文件是处理过的
3. 1和2的结果进行对比，找出哪些文件没有被处理，可以用于本次采集
4. 读取文件的每一行
5. 将每一行转换成Model
6. 将model调用生成sql语句，插入mysql
7. 将model调用to_csv写出csv
8. 记录元数据，本次哪些文件被处理了
9. 结束



在model内创建：`backend_logs_model.py`

```python
# coding:utf8
"""
后台日志采集的model模型
"""
from config import project_config as conf


class BackendLogsModel:
    def __init__(self, data: str, sep="\t"):
        arrs = data.split(sep)

        self.log_time = arrs[0]                                     # 日志时间
        self.log_level = arrs[1].replace("[", "").replace("]", "")  # 日志级别,处理方括号
        self.log_module = arrs[2]                                   # 日志模块
        self.response_time = int(arrs[3][:-2][5:])                  # 响应时间提取
        self.province = arrs[4]                                     # 省份
        self.city = arrs[5]                                         # 城市
        self.log_text = arrs[6]                                     # 日志正文

    def to_string(self):
        return f"log_time: {self.log_time}, " \
                f"log_level: {self.log_level}, " \
                f"log_module: {self.log_module}, " \
                f"response_time: {self.response_time}, " \
                f"province: {self.province}, " \
                f"city: {self.city}, " \
                f"log_text: {self.log_text}"

    def generate_insert_sql(self):
        # 注意，表名自行配置到配置文件中
        return f"INSERT INTO backend_logs(" \
                f"log_time, log_level, log_module, response_time, province, city, log_text) VALUES(" \
                f"'{self.log_time}', " \
                f"'{self.log_level}', " \
                f"'{self.log_module}', " \
                f"{self.response_time}, " \
                f"'{self.province}', " \
                f"'{self.city}', " \
                f"'{self.log_text}'" \
                f")"

    def to_csv(self, sep=","):
        return \
            f"{self.log_time}{sep}" \
            f"{self.log_level}{sep}" \
            f"{self.log_module}{sep}" \
            f"{self.response_time}{sep}" \
            f"{self.province}{sep}" \
            f"{self.city}{sep}" \
            f"{self.log_text}"

```







在工程根目录创建：`backend_logs_service.py`



```python
# coding:utf8
"""
需求3：主业务逻辑文件，采集后台日志
"""
import time
from util import file_util as fu
from util.mysql_util import MySQLUtil, get_processed_files
from config import project_config as conf
from model.backend_logs_model import BackendLogsModel

# TODO 步骤1：读取文件夹中有哪些文件
# 路径请自行改到配置文件中
files = fu.get_dir_files_list("E:/pyetl-data-logs/backend_logs")

# TODO 步骤2： 读取mysql元数据库
# 构建元数据库连接db_util对象
metadata_db_util = MySQLUtil()

# 构建目标库的db_util对象
target_db_util = MySQLUtil(
    conf.target_host,
    user=conf.target_user,
    password=conf.target_password,
    port=conf.target_port,
    charset=conf.mysql_charset,
    autocommit=False
)
# 检查要写入数据的目标表是否存在，不存在创建
# 表名和建表列信息，自行改到配置文件中
target_db_util.check_table_exists_and_create(
    conf.target_db_name,
    "backend_logs",
    create_cols= \
        f"id int PRIMARY KEY AUTO_INCREMENT COMMENT '自增ID', " \
        f"log_time TIMESTAMP(6) COMMENT '日志时间,精确到6位毫秒值', " \
        f"log_level VARCHAR(10) COMMENT '日志级别', " \
        f"log_module VARCHAR(50) COMMENT '输出日志的功能模块名', " \
        f"response_time INT COMMENT '接口响应时间毫秒', " \
        f"province VARCHAR(30) COMMENT '访问者省份', " \
        f"city VARCHAR(30) COMMENT '访问者城市', " \
        f"log_text VARCHAR(255) COMMENT '日志正文', " \
        f"INDEX(log_time)"
)


# 检查元数据的表是否存在
# 表名和建表的列信息，请自行改到配置文件中
# 工具方法获得MySQL中有哪些数据被处理了
# get_processed_files方法 会自动创建表，如果不存在
processed_files = get_processed_files(
    db_util=metadata_db_util,
    db_name=conf.metadata_db_name,
    table_name="backend_logs_monitor",
    create_cols="id INT PRIMARY KEY AUTO_INCREMENT, " \
    "file_name VARCHAR(255) NOT NULL COMMENT '处理文件名称', " \
    "process_lines INT NULL COMMENT '文件处理行数', " \
    "process_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT '文件处理时间'"
)

# TODO 步骤3：对比找出哪些文件可以被处理
# need需要 process 处理
need_to_process_files = fu.get_new_by_compare_lists(processed_files, files)

# 构建一个文件对象，用于写出CSV
# 写出的路径请自行更改到配置文件中
backend_logs_write_f = open(
    "E:/pyetl-data-logs/output/csv/" + f'backend-logs-{time.strftime("%Y%m%d-%H%M%S", time.localtime(time.time()))}.csv',
    "a",
    encoding="UTF-8"
)

# 全局计数器
global_count = 0
# 构建一个字典，记录每一个文件被处理的行数
processed_files_record_dict = {}
# TODO 步骤4： 读取文件每一行进行处理
for file in need_to_process_files:
    single_file_count = 0       # 针对每一个文件被处理的行数的计数器
    for line in open(file, "r", encoding="UTF-8"):
        line = line.replace("\n", "")   # 处理掉每一行的\n回车符
        # TODO 步骤5：构建模型
        model = BackendLogsModel(line)

        # TODO 步骤6：写入MySQL（不要忘记构建目标库的db_util对象，以及检查目标表是否存在，不存在创建，参见19~42行）
        insert_sql = model.generate_insert_sql()
        target_db_util.select_db(conf.target_db_name)
        target_db_util.execute_without_autocommit(insert_sql)

        # TODO 步骤7：写出CSV（不要忘记构建写出CSV的文件对象）
        backend_logs_write_f.write(model.to_csv())
        backend_logs_write_f.write("\n")

        global_count += 1
        single_file_count += 1
        if global_count % 1000 == 0:
            # 提交MySQL
            target_db_util.conn.commit()
            # 刷新文件写出的缓冲区
            backend_logs_write_f.flush()

    processed_files_record_dict[file] = single_file_count


target_db_util.conn.commit()
backend_logs_write_f.close()

# TODO 步骤8：记录元数据
for file_name in processed_files_record_dict.keys():
    count = processed_files_record_dict[file_name]
    sql = f"INSERT INTO backend_logs_monitor(file_name, process_lines) VALUES(" \
          f"'{file_name}', {count}" \
          f")"
    metadata_db_util.select_db(conf.metadata_db_name)
    metadata_db_util.execute(sql)

metadata_db_util.close_conn()
target_db_util.close_conn()

```



## 主业务逻辑代码梳理

1：

![image-20220519180750379](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519180750.png)



2：

![image-20220519180848056](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519180848.png)



3：

![image-20220519181019285](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519181019.png)



4：

![image-20220519181250601](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519181250.png)



5：

![image-20220519181429899](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519181429.png)



6：

![image-20220519181705560](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519181705.png)



7：

![image-20220519181815916](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519181816.png)



8：

![image-20220519181905194](https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2022/05/19/20220519181905.png)

















